"""
GÃ¼rÃ¼ltÃ¼ Ã–lÃ§Ã¼m Raporu Analiz Servisi
====================================
Azure App Service iÃ§in optimize edilmiÅŸ standalone servis

Bu dosya ÅŸunlarÄ± iÃ§erir:
1. NoiseReportAnalyzer sÄ±nÄ±fÄ± (noise_report_checker.py)
2. Flask API servisi (server4.py)
3. Azure-friendly konfigÃ¼rasyon
4. Database entegrasyonu (TAM BAÄIMLI - Flashback yok)

Endpoint: POST /api/noise-report
Health Check: GET /api/health
"""

# ============================================
# IMPORTS
# ============================================
import re
import os
import json
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any
import PyPDF2
from docx import Document
from dataclasses import dataclass, asdict
import logging
import cv2
import numpy as np
from PIL import Image
import pdf2image
import pytesseract

from flask import Flask, request, jsonify
from werkzeug.utils import secure_filename

# ============================================
# DATABASE IMPORTS (YENÄ°)
# ============================================
from flask import current_app
from database import db, init_db
from db_loader import load_service_config

# ============================================
# CONFIG IMPORT (YENÄ°)
# ============================================
from config import Config

# ============================================
# LOGGING CONFIGURATION
# ============================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# ============================================
# ANALÄ°Z SINIFI - DATA CLASSES
# ============================================
@dataclass
class NoiseCriteria:
    rapor_kimlik_bilgileri: Dict[str, Any]
    olcum_ortam_ekipman: Dict[str, Any]
    olcum_cihazi_bilgileri: Dict[str, Any]
    olcum_metodolojisi: Dict[str, Any]
    olcum_sonuclari: Dict[str, Any]
    degerlendirme_yorum: Dict[str, Any]
    ekler_gorseller: Dict[str, Any]

@dataclass
class NoiseAnalysisResult:
    criteria_name: str
    found: bool
    content: str
    score: int
    max_score: int
    details: Dict[str, Any]


# ============================================
# ANALÄ°Z SINIFI - MAIN ANALYZER
# ============================================
class NoiseReportAnalyzer:
    
    def __init__(self, app=None):
        """GÃ¼rÃ¼ltÃ¼ Ã–lÃ§Ã¼m Raporu analiz sistemi baÅŸlatÄ±lÄ±yor - Sadece Database'den yÃ¼kleme"""
        logger.info("GÃ¼rÃ¼ltÃ¼ Ã–lÃ§Ã¼m Raporu analiz sistemi baÅŸlatÄ±lÄ±yor...")
        
        if not app:
            raise ValueError("âŒ Flask app context gerekli! Analyzer'Ä± app ile baÅŸlatÄ±n: NoiseReportAnalyzer(app=app)")
        
        with app.app_context():
            config = load_service_config('noise_report')
            
            # DB'den yÃ¼klenen veriler
            self.criteria_weights = config.get('criteria_weights', {})
            self.criteria_details = config.get('criteria_details', {})
            self.pattern_definitions = config.get('pattern_definitions', {})
            self.validation_keywords = config.get('validation_keywords', {})
            self.category_actions = config.get('category_actions', {})
            
            # Kritik verilerin varlÄ±ÄŸÄ±nÄ± kontrol et
            if not self.criteria_weights:
                raise ValueError("âŒ DB'den criteria_weights yÃ¼klenemedi!")
            if not self.validation_keywords:
                raise ValueError("âŒ DB'den validation_keywords yÃ¼klenemedi!")
            
            logger.info(f"âœ… VeritabanÄ±ndan yÃ¼klendi: {len(self.criteria_weights)} kategori")
            logger.info(f"âœ… Validation keywords: {len(self.validation_keywords)} tip")
            logger.info(f"âœ… Pattern definitions: {len(self.pattern_definitions)} grup")
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
                return text
        except Exception as e:
            logger.error(f"PDF okuma hatasÄ±: {e}")
            return ""
    
    def check_report_date_validity(self, text: str) -> Tuple[bool, str, str]:
        """Rapor tarihini kontrol et - DB'den pattern'ler ile"""
        # DB'den date extraction patterns al
        date_patterns = self.pattern_definitions.get('date_extraction', {}).get('date_patterns', [])
        

        if not date_patterns:
            logger.error("âŒ DB'den date_patterns yÃ¼klenemedi!")
            return False, "", "Tarih pattern'leri bulunamadÄ±"
        
        for pattern in date_patterns:
            matches = re.findall(pattern, text)
            if matches:
                date_str = matches[0]
                try:
                    date_str = date_str.replace('.', '/').replace('-', '/')
                    report_date = datetime.strptime(date_str, '%d/%m/%Y')
                    one_year_ago = datetime.now() - timedelta(days=365)
                    is_valid = report_date >= one_year_ago
                    return is_valid, date_str, f"Rapor tarihi: {date_str} {'(GEÃ‡ERLÄ°)' if is_valid else '(GEÃ‡ERSÄ°Z - 1 yÄ±ldan eski)'}"
                except ValueError:
                    continue
                
        return False, "", "Rapor tarihi bulunamadÄ±"
    
    def analyze_criteria(self, text: str, category: str) -> Dict[str, NoiseAnalysisResult]:
        """Kriterleri analiz et - DB'den pattern'ler ile"""
        results = {}
        criteria = self.criteria_details.get(category, {})
        
        for criterion_name, criterion_data in criteria.items():
            pattern = criterion_data["pattern"]
            weight = criterion_data["weight"]
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            
            if matches:
                content = str(matches[0]) if len(matches) == 1 else str(matches)
                found = True
                score = weight
            else:
                content = "BulunamadÄ±"
                found = False
                score = 0
            
            results[criterion_name] = NoiseAnalysisResult(
                criteria_name=criterion_name,
                found=found,
                content=content,
                score=score,
                max_score=weight,
                details={"pattern_used": pattern, "matches_found": len(matches) if matches else 0}
            )
        
        return results
    
    def extract_specific_values(self, text: str) -> Dict[str, Any]:
        """GÃ¼rÃ¼ltÃ¼ raporuna Ã¶zgÃ¼ deÄŸerleri Ã§Ä±kar - DB'den pattern'leri kullanarak"""
        values = {}
        
        # DB'den extract_values pattern'lerini al
        extract_patterns = self.pattern_definitions.get('extract_values', {})
        
        if not extract_patterns:
            logger.warning("âš ï¸ DB'den extract_values pattern'leri yÃ¼klenemedi!")
            return {}
        
        # Her field iÃ§in pattern'leri dene
        for key, patterns_list in extract_patterns.items():
            found = False
            for pattern in patterns_list:
                matches = re.findall(pattern, text, re.IGNORECASE)
                if matches:
                    values[key] = matches[0].strip()
                    found = True
                    break
            
            if not found:
                values[key] = "BulunamadÄ±"
        
        return values
    
    def calculate_scores(self, analysis_results: Dict[str, Dict[str, NoiseAnalysisResult]]) -> Dict[str, Any]:
        """PuanlarÄ± hesapla"""
        category_scores = {}
        total_score = 0
        
        for category, results in analysis_results.items():
            category_max = self.criteria_weights[category]
            category_earned = sum(result.score for result in results.values())
            category_possible = sum(result.max_score for result in results.values())
            
            normalized_score = (category_earned / category_possible * category_max) if category_possible > 0 else 0
            
            category_scores[category] = {
                "earned": category_earned,
                "possible": category_possible,
                "normalized": round(normalized_score, 2),
                "max_weight": category_max,
                "percentage": round((category_earned / category_possible * 100), 2) if category_possible > 0 else 0
            }
            
            total_score += normalized_score
        
        return {
            "category_scores": category_scores,
            "total_score": round(total_score, 2),
            "total_max_score": 100,
            "overall_percentage": round((total_score / 100 * 100), 2)
        }
    
    def generate_recommendations(self, analysis_results: Dict, scores: Dict, date_valid: bool = True, date_message: str = "") -> List[str]:
        """Ã–neriler oluÅŸtur"""
        recommendations = []

        # TARÄ°H KONTROLÃœ - EÄŸer tarih geÃ§ersizse SADECE BUNU GÃ–R
        if not date_valid:
            recommendations.append(f"âŒ {date_message}")
            return recommendations  # â† DÄ°ÄER Ã–NERÄ°LERÄ° ATLAT!

        # TARÄ°H KONTROLÃœ 
        if not date_valid:
            recommendations.append(f"âŒ {date_message}")
        
        for category, results in analysis_results.items():
            category_score = scores["category_scores"][category]["percentage"]
            
            if category_score < 50:
                recommendations.append(f"âŒ {category} bÃ¶lÃ¼mÃ¼ yetersiz (%{category_score:.1f})")
            elif category_score < 80:
                recommendations.append(f"âš ï¸ {category} bÃ¶lÃ¼mÃ¼ geliÅŸtirilmeli (%{category_score:.1f})")
            else:
                recommendations.append(f"âœ… {category} bÃ¶lÃ¼mÃ¼ yeterli (%{category_score:.1f})")
        
        if scores["overall_percentage"] < 70:
            recommendations.append("\nğŸš¨ GENEL Ã–NERÄ°LER:")
            recommendations.append("- Rapor ISO 11201, ISO 9612 standartlarÄ±na tam uyumlu hale getirilmelidir")
        
        return recommendations
    
    def generate_detailed_report(self, pdf_path: str, docx_path: str = None) -> Dict[str, Any]:
        """DetaylÄ± rapor oluÅŸtur"""
        logger.info("GÃ¼rÃ¼ltÃ¼ Ã¶lÃ§Ã¼m raporu analizi baÅŸlatÄ±lÄ±yor...")
        
        pdf_text = self.extract_text_from_pdf(pdf_path)
        if not pdf_text:
            return {"error": "PDF okunamadÄ±"}
        
        date_valid, date_str, date_message = self.check_report_date_validity(pdf_text)
        extracted_values = self.extract_specific_values(pdf_text)
        
        analysis_results = {}
        for category in self.criteria_weights.keys():
            analysis_results[category] = self.analyze_criteria(pdf_text, category)
        
        scores = self.calculate_scores(analysis_results)
        recommendations = self.generate_recommendations(analysis_results, scores, date_valid, date_message)
        
        report = {
            "analiz_tarihi": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "tarih_gecerliligi": {
                "gecerli": date_valid,
                "tarih": date_str,
                "mesaj": date_message
            },
            "cikarilan_degerler": extracted_values,
            "kategori_analizleri": analysis_results,
            "puanlama": scores,
            "oneriler": recommendations,
            "ozet": {
                "toplam_puan": scores["total_score"],
                "yuzde": scores["overall_percentage"],
                "durum": "GEÃ‡ERLÄ°" if scores["overall_percentage"] >= 70 else "YETERSÄ°Z",
                "tarih_durumu": "GEÃ‡ERLÄ°" if date_valid else "GEÃ‡ERSÄ°Z"
            }
        }
        
        return report


# ============================================
# HELPER FUNCTIONS (Server Validasyon)
# ============================================
def validate_document_server(text, validation_keywords):
    """GÃ¼rÃ¼ltÃ¼ dokÃ¼man validasyonu - DB'den gelen keywords ile"""
    
    # DB'den gelen critical_terms
    critical_terms_data = validation_keywords.get('critical_terms', [])
    
    if not critical_terms_data:
        logger.error("âŒ DB'den critical_terms yÃ¼klenemedi!")
        raise ValueError("Critical terms bulunamadÄ± - DB kontrolÃ¼ gerekli")
    
    # Liste formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
    critical_terms = []
    for item in critical_terms_data:
        if isinstance(item, dict) and 'keywords' in item:
            critical_terms.append(item['keywords'])
        elif isinstance(item, list):
            critical_terms.append(item)
    
    category_found = [any(re.search(rf"\b{term}\b", text, re.IGNORECASE) for term in category) for category in critical_terms]
    valid = sum(category_found)
    logger.info(f"GÃ¼rÃ¼ltÃ¼ validasyon: {valid}/{len(critical_terms)} kategori")
    return valid >= 4


def check_strong_keywords_first_pages(filepath, validation_keywords):
    """Ä°lk sayfada gÃ¼rÃ¼ltÃ¼ Ã¶zgÃ¼ kelime kontrolÃ¼ - OCR - DB'den keywords"""
    
    # DB'den strong keywords al
    strong_keywords = validation_keywords.get('strong_keywords', [])
    
    if not strong_keywords:
        logger.error("âŒ DB'den strong_keywords yÃ¼klenemedi!")
        raise ValueError("Strong keywords bulunamadÄ± - DB kontrolÃ¼ gerekli")
    
    try:
        Image.MAX_IMAGE_PIXELS = None
        pages = pdf2image.convert_from_path(filepath, dpi=150, first_page=1, last_page=1)
        
        all_text = ""
        for page in pages:
            if page.size[0] > 2000 or page.size[1] > 2000:
                page = page.resize((1500, int(1500 * page.size[1] / page.size[0])), Image.Resampling.LANCZOS)
            opencv_image = cv2.cvtColor(np.array(page), cv2.COLOR_RGB2BGR)
            gray = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2GRAY)
            processed = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
            text = pytesseract.image_to_string(processed, config='--oem 3 --psm 6 -l tur+eng')
            all_text += text.lower() + " "
        
        found = [kw for kw in strong_keywords if re.search(rf"\b{kw.lower()}\b", all_text)]
        logger.info(f"Ä°lk sayfa: {len(found)} gÃ¼rÃ¼ltÃ¼ kelime bulundu")
        return len(found) >= 1
    except Exception as e:
        logger.warning(f"OCR hatasÄ±: {e}")
        return False


def check_excluded_keywords_first_pages(filepath, validation_keywords):
    """Ä°lk sayfada excluded keyword kontrolÃ¼ - OCR - DB'den keywords"""
    
    # DB'den excluded keywords al
    excluded_keywords = validation_keywords.get('excluded_keywords', [])
    
    if not excluded_keywords:
        logger.error("âŒ DB'den excluded_keywords yÃ¼klenemedi!")
        raise ValueError("Excluded keywords bulunamadÄ± - DB kontrolÃ¼ gerekli")
    
    try:
        Image.MAX_IMAGE_PIXELS = None
        pages = pdf2image.convert_from_path(filepath, dpi=400, first_page=1, last_page=2)
        
        all_text = ""
        for page in pages:
            if page.size[0] > 2000 or page.size[1] > 2000:
                page = page.resize((1500, int(1500 * page.size[1] / page.size[0])), Image.Resampling.LANCZOS)
            opencv_image = cv2.cvtColor(np.array(page), cv2.COLOR_RGB2BGR)
            gray = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2GRAY)
            processed = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
            text = pytesseract.image_to_string(processed, config='--oem 3 --psm 6 -l tur+eng')
            all_text += text.lower() + " "
        
        found = [kw for kw in excluded_keywords if re.search(rf"\b{kw.lower()}\b", all_text)]
        logger.info(f"Excluded: {len(found)} kelime bulundu")
        return len(found) >= 1
    except Exception as e:
        logger.warning(f"Excluded OCR hatasÄ±: {e}")
        return False


def get_conclusion_message_noise(status, percentage):
    """SonuÃ§ mesajÄ± - GÃ¼rÃ¼ltÃ¼"""
    if status == "PASS":
        return f"GÃ¼rÃ¼ltÃ¼ Ã¶lÃ§Ã¼m raporu standartlara uygun (%{percentage:.0f})"
    return f"GÃ¼rÃ¼ltÃ¼ Ã¶lÃ§Ã¼m raporu standartlara uygun deÄŸil (%{percentage:.0f})"


def get_main_issues_noise(report):
    """Ana sorunlar - GÃ¼rÃ¼ltÃ¼"""
    issues = []
    for category, score_data in report["puanlama"]["category_scores"].items():
        if score_data.get('percentage', 0) < 50:
            issues.append(f"{category} kategorisinde ciddi eksiklikler")
    return issues[:4]


# ============================================
# TESSERACT CHECK
# ============================================
def check_tesseract_installation():
    """Tesseract kontrolÃ¼"""
    try:
        version = pytesseract.get_tesseract_version()
        logger.info(f"Tesseract OCR kurulu - SÃ¼rÃ¼m: {version}")
        return True, f"Tesseract v{version}"
    except Exception as e:
        logger.error(f"Tesseract OCR kurulu deÄŸil: {e}")
        return False, str(e)

tesseract_available, tesseract_info = check_tesseract_installation()


# ============================================
# FLASK SERVÄ°S KATMANI - CONFIGURATION
# ============================================
app = Flask(__name__)

# Database configuration (YENÄ°)
app.config['SQLALCHEMY_DATABASE_URI'] = Config.SQLALCHEMY_DATABASE_URI
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = Config.SQLALCHEMY_TRACK_MODIFICATIONS
app.config['SQLALCHEMY_ECHO'] = Config.SQLALCHEMY_ECHO

# Upload configuration
UPLOAD_FOLDER = 'temp_uploads_noise'
ALLOWED_EXTENSIONS = {'pdf', 'jpg', 'jpeg', 'png'}

if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 32 * 1024 * 1024


def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


# ============================================
# FLASK SERVÄ°S KATMANI - API ENDPOINTS
# ============================================
@app.route('/api/noise-report', methods=['POST'])
def analyze_noise_report():
    """GÃ¼rÃ¼ltÃ¼ Ã–lÃ§Ã¼m Raporu analiz endpoint'i"""
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'No file provided'}), 400

        file = request.files['file']
        if file.filename == '' or not allowed_file(file.filename):
            return jsonify({'error': 'Invalid file'}), 400

        try:
            filename = secure_filename(file.filename)
            filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            file.save(filepath)
            logger.info(f"GÃ¼rÃ¼ltÃ¼ analizi baÅŸlatÄ±lÄ±yor: {filename}")

            # Analyzer'Ä± app ile baÅŸlat (DB baÄŸlantÄ±sÄ± iÃ§in)
            analyzer = NoiseReportAnalyzer(app=app)
            file_ext = os.path.splitext(filepath)[1].lower()
            
            # 3 AÅAMALI KONTROL
            if file_ext == '.pdf':
                logger.info("AÅŸama 1: GÃ¼rÃ¼ltÃ¼ Ã¶zgÃ¼ kelime kontrolÃ¼...")
                if check_strong_keywords_first_pages(filepath, analyzer.validation_keywords):
                    logger.info("âœ… AÅŸama 1 geÃ§ti")
                else:
                    logger.info("AÅŸama 2: Excluded kelime kontrolÃ¼...")
                    if check_excluded_keywords_first_pages(filepath, analyzer.validation_keywords):
                        logger.info("âŒ Excluded kelimeler bulundu")
                        try:
                            os.remove(filepath)
                        except:
                            pass
                        return jsonify({
                            'error': 'Invalid document type',
                            'message': 'Bu dosya gÃ¼rÃ¼ltÃ¼ Ã¶lÃ§Ã¼m raporu deÄŸil'
                        }), 400
                    else:
                        # AÅAMA 3
                        logger.info("AÅŸama 3: Tam dokÃ¼man kontrolÃ¼...")
                        try:
                            with open(filepath, 'rb') as f:
                                pdf_reader = PyPDF2.PdfReader(f)
                                text = "".join(page.extract_text() + "\n" for page in pdf_reader.pages)
                            
                            if not text or len(text.strip()) < 50 or not validate_document_server(text, analyzer.validation_keywords):
                                try:
                                    os.remove(filepath)
                                except:
                                    pass
                                return jsonify({
                                    'error': 'Invalid document type',
                                    'message': 'YÃ¼klediÄŸiniz dosya gÃ¼rÃ¼ltÃ¼ Ã¶lÃ§Ã¼m raporu deÄŸil!'
                                }), 400
                        except Exception as e:
                            logger.error(f"AÅŸama 3 hatasÄ±: {e}")
                            try:
                                os.remove(filepath)
                            except:
                                pass
                            return jsonify({'error': 'Analysis failed'}), 500

            # Analizi yap
            logger.info(f"GÃ¼rÃ¼ltÃ¼ analizi yapÄ±lÄ±yor: {filename}")
            full_report = analyzer.generate_detailed_report(filepath, None)
            
            try:
                os.remove(filepath)
            except:
                pass

            if 'error' in full_report:
                return jsonify({'error': 'Analysis failed', 'message': full_report['error']}), 400

            overall_percentage = full_report["ozet"]["yuzde"]
            status = "PASS" if overall_percentage >= 70 else "FAIL"
            
            response_data = {
                'analysis_date': full_report["analiz_tarihi"],
                'analysis_details': {'found_criteria': len([c for c in full_report["puanlama"]["category_scores"].values() if c.get('percentage', 0) > 50])},
                'analysis_id': f"noise_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                'category_scores': {},
                'date_validity': {
                    'is_valid': full_report["tarih_gecerliligi"]["gecerli"],
                    'message': full_report["tarih_gecerliligi"]["mesaj"]
                },
                'extracted_values': full_report["cikarilan_degerler"],
                'file_type': 'GURULTU_OLCUM_RAPORU',
                'filename': filename,
                'language_info': {'detected_language': 'turkish', 'text_length': 0},
                'overall_score': {
                    'percentage': round(overall_percentage, 2),
                    'total_points': full_report["ozet"]["toplam_puan"],
                    'max_points': 100,
                    'status': status,
                    'status_tr': 'GEÃ‡ERLÄ°' if status == "PASS" else 'GEÃ‡ERSÄ°Z',
                    'text_quality': 'good'
                },
                'recommendations': full_report["oneriler"],
                'summary': {
                    'is_valid': status == "PASS",
                    'conclusion': get_conclusion_message_noise(status, overall_percentage),
                    'main_issues': [] if status == "PASS" else get_main_issues_noise(full_report)
                }
            }
            
            for kategori, score_data in full_report["puanlama"]["category_scores"].items():
                response_data['category_scores'][kategori] = {
                    "score": score_data["normalized"],
                    "max_score": score_data["max_weight"],
                    "percentage": round(score_data["percentage"], 1),
                    'status': 'PASS' if score_data["percentage"] >= 70 else 'FAIL'
                }

            return jsonify({
                'success': True,
                'message': 'GÃ¼rÃ¼ltÃ¼ Ã–lÃ§Ã¼m Raporu baÅŸarÄ±yla analiz edildi',
                'analysis_service': 'noise_report',
                'service_description': 'GÃ¼rÃ¼ltÃ¼ Ã–lÃ§Ã¼m Rapor Analizi',
                'data': response_data
            })

        except Exception as analysis_error:
            try:
                if 'filepath' in locals():
                    os.remove(filepath)
            except:
                pass
            logger.error(f"Analiz hatasÄ±: {str(analysis_error)}")
            return jsonify({'error': 'Analysis failed', 'message': str(analysis_error)}), 500

    except Exception as e:
        logger.error(f"API hatasÄ±: {str(e)}")
        return jsonify({'error': 'Server error', 'message': str(e)}), 500


@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'service': 'Noise Report Analyzer API',
        'version': '1.0.0',
        'report_type': 'GURULTU_OLCUM_RAPORU',
        'database': 'connected'
    })


@app.route('/', methods=['GET'])
def index():
    """API bilgileri"""
    return jsonify({
        'service': 'Noise Report Analyzer API',
        'version': '1.0.0',
        'description': 'GÃ¼rÃ¼ltÃ¼ Ã¶lÃ§Ã¼m raporlarÄ±nÄ± analiz eden REST API servisi',
        'database': 'PostgreSQL (Dynamic config loading)',
        'endpoints': {
            'POST /api/noise-report': 'GÃ¼rÃ¼ltÃ¼ Ã¶lÃ§Ã¼m raporu analizi',
            'GET /api/health': 'Servis saÄŸlÄ±k kontrolÃ¼',
            'GET /': 'Bu bilgi sayfasÄ±'
        }
    })


# ============================================
# DATABASE INITIALIZATION (YENÄ°)
# ============================================
with app.app_context():
    db.init_app(app)
    logger.info("âœ… Database baÄŸlantÄ±sÄ± kuruldu")


# ============================================
# APPLICATION ENTRY POINT (Azure-Friendly)
# ============================================
if __name__ == '__main__':
    logger.info("=" * 60)
    logger.info("GÃ¼rÃ¼ltÃ¼ Ã–lÃ§Ã¼m Raporu Analiz Servisi")
    logger.info("=" * 60)
    
    port = int(os.environ.get('PORT', 8003))
    
    logger.info(f"ğŸš€ Servis baÅŸlatÄ±lÄ±yor - Port: {port}")
    logger.info(f"ğŸ“Š Database: PostgreSQL (Config from DB)")
    logger.info("=" * 60)
    
    app.run(host='0.0.0.0', port=port, debug=False)