from dotenv import load_dotenv

# âœ… .env dosyasÄ±nÄ± yÃ¼kle (EN Ã–NCE!)
load_dotenv()

# ============================================
# IMPORTS - STANDARD LIBRARIES
# ============================================
from flask import Flask, request, jsonify, render_template
from werkzeug.utils import secure_filename
import os
import json
from datetime import datetime, timezone
import logging
import atexit
import time
import threading
from typing import Dict, List, Any 

from database import db, init_db
from models import DocumentType, Ticket, TicketComment, CriteriaWeight, CriteriaDetail, PatternDefinition, ValidationKeyword, CategoryAction
import anthropic

# ============================================
# AI-POWERED PATTERN UPDATE CONFIGURATION
# ============================================
AUTHORIZED_USERS = ["SavaÅŸ", "Kayra", "Can", "Utku"]

# ============================================
# LOGGING CONFIGURATION
# ============================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ============================================
# FILE BASE URL (External System)
# ============================================
FILE_BASE_URL = "https://safetyexpert.app/fileupload/Account_103/Machine_4879/"

DOCUMENT_HANDLERS = {}


def load_document_handlers():
    """
    Database'den document type'larÄ± ve service import'larÄ±nÄ± yÃ¼kle
    """
    global DOCUMENT_HANDLERS
    
    try:
        from models import DocumentType
        
        doc_types = DocumentType.query.filter_by(is_active=True).all()
        
        # Dynamic service'i bir kere yÃ¼kle
        dynamic_app = None
        
        for dt in doc_types:
            try:
                # Dynamic service kontrolÃ¼
                if dt.service_file == 'dynamic_service.py':
                    # Dynamic service'i lazy load yap
                    if dynamic_app is None:
                        logger.info("ğŸ”„ Dynamic service yÃ¼kleniyor...")
                        dynamic_module = __import__('dynamic_service', fromlist=['app'])
                        dynamic_app = getattr(dynamic_module, 'app')
                        logger.info("âœ… Dynamic service yÃ¼klendi")
                    
                    DOCUMENT_HANDLERS[dt.code] = {
                        'app': dynamic_app,
                        'endpoint': dt.endpoint,
                        'description': dt.description,
                        'is_dynamic': True  # Flag ekle
                    }
                    
                    logger.info(f"âœ… Loaded (dynamic): {dt.code} â†’ {dt.name}")
                    
                else:
                    # Normal servisler (eski)
                    module_name = dt.service_file.replace('.py', '')
                    module = __import__(module_name, fromlist=['app'])
                    service_app = getattr(module, 'app')
                    
                    DOCUMENT_HANDLERS[dt.code] = {
                        'app': service_app,
                        'endpoint': dt.endpoint,
                        'description': dt.description,
                        'is_dynamic': False
                    }
                    
                    logger.info(f"âœ… Loaded: {dt.code} â†’ {module_name}")
                
            except Exception as e:
                logger.error(f"âŒ {dt.code} import hatasÄ±: {str(e)}")
        
        logger.info(f"ğŸ“‹ Total handlers loaded: {len(DOCUMENT_HANDLERS)}")
        
    except Exception as e:
        logger.error(f"âŒ Document handlers yÃ¼klenemedi: {str(e)}")
        raise


# ============================================
# FLASK APP CONFIGURATION
# ============================================
app = Flask(__name__)

# Upload settings
UPLOAD_FOLDER = 'temp_uploads_main'
ALLOWED_EXTENSIONS = {'pdf', 'jpg', 'jpeg', 'png'}

if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 32 * 1024 * 1024  # 32MB


# ============================================
# HELPER FUNCTIONS
# ============================================
def allowed_file(filename):
    """Dosya uzantÄ±sÄ± kontrolÃ¼"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


def generate_ticket_no():
    """Yeni ticket numarasÄ± oluÅŸtur (sÄ±ralÄ±)"""
    last_ticket = Ticket.query.order_by(Ticket.id.desc()).first()
    if last_ticket:
        # Son ticket_no'dan sayÄ±yÄ± Ã§Ä±kar (Ã¶rn: "ticket5" -> 5)
        try:
            last_num = int(last_ticket.ticket_no.replace('ticket', ''))
            return f"ticket{last_num + 1}"
        except:
            return f"ticket{Ticket.query.count() + 1}"
    return "ticket1"


# ============================================
# MAIN ANALYSIS ENDPOINT
# ============================================
def analyze_document_async(file_data, filename, document_type, handler_info, custom_document_url, initial_comment, comment_author, ticket_id):
    """
    Arka planda analiz yapan fonksiyon (thread iÃ§inde Ã§alÄ±ÅŸÄ±r)
    """
    try:
        logger.info(f"ğŸ”„ Async analiz baÅŸladÄ±: {ticket_id}")
        
        # DosyayÄ± geÃ§ici olarak kaydet
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        with open(filepath, 'wb') as f:
            f.write(file_data)
        
        # Service'e gÃ¶nder
        service_app = handler_info['app']
        service_endpoint = handler_info['endpoint']
        is_dynamic = handler_info.get('is_dynamic', False)
        
        with service_app.test_client() as client:
            with open(filepath, 'rb') as file_to_send:
                form_data = {
                    'file': (file_to_send, filename),
                }
                
                if is_dynamic:
                    form_data['document_code'] = document_type
                
                response = client.post(
                    service_endpoint,
                    data=form_data,
                    content_type='multipart/form-data'
                )
        
        # GeÃ§ici dosyayÄ± sil
        try:
            os.remove(filepath)
        except:
            pass
        
        result = response.get_json()
        
        # Ticket'Ä± gÃ¼ncelle
        with app.app_context():
            ticket = Ticket.query.filter_by(ticket_id=ticket_id).first()
            
            if ticket and result and 'data' in result:
                analysis_data = result['data']
                
                # Analiz sonucunu kaydet
                ticket.analysis_result = {
                    'overall_score': analysis_data.get('overall_score', {}),
                    'category_scores': analysis_data.get('category_scores', {}),
                    'extracted_values': analysis_data.get('extracted_values', {}),
                    'recommendations': analysis_data.get('recommendations', []),
                    'summary': analysis_data.get('summary', '')
                }
                
                # Status gÃ¼ncelle
                if initial_comment:
                    ticket.status = 'Ä°nceleniyor'
                    # Ä°lk yorumu ekle
                    first_comment = TicketComment(
                        ticket_id=ticket.id,
                        comment_id='comment_1',
                        author=comment_author if comment_author else 'Anonim KullanÄ±cÄ±',
                        text=initial_comment,
                        timestamp=datetime.now()
                    )
                    db.session.add(first_comment)
                else:
                    ticket.status = 'KapalÄ±'
                    ticket.closing_date = datetime.now()
                
                ticket.last_updated = datetime.now()
                
                db.session.commit()
                
                logger.info(f"âœ… Async analiz tamamlandÄ±: {ticket_id} - Status: {ticket.status}")
            
    except Exception as e:
        logger.error(f"âŒ Async analiz hatasÄ± ({ticket_id}): {str(e)}")
        
        # Hata durumunda ticket'Ä± gÃ¼ncelle
        try:
            with app.app_context():
                ticket = Ticket.query.filter_by(ticket_id=ticket_id).first()
                if ticket:
                    ticket.status = 'HatalÄ±'
                    ticket.analysis_result = {'error': str(e)}
                    ticket.last_updated = datetime.now()
                    db.session.commit()
        except:
            pass

@app.route('/api/analyze', methods=['POST'])
def analyze_document():
    """
    Ana analiz endpoint'i - ASYNC (PostgreSQL ile)
    
    POST /api/analyze
    {
        "file": <binary>,
        "document_type": "vibration_report",
        "initial_comment": "...",  # opsiyonel
        "comment_author": "..."    # opsiyonel
    }
    """
    try:
        # 1. DOSYA KONTROLÃœ
        if 'file' not in request.files:
            return jsonify({
                'error': 'No file provided',
                'message': 'Please provide a file in the request'
            }), 400

        file = request.files['file']
        
        if file.filename == '':
            return jsonify({
                'error': 'No file selected',
                'message': 'Please select a file to upload'
            }), 400

        if not allowed_file(file.filename):
            return jsonify({
                'error': 'Invalid file type',
                'message': 'Only PDF, JPG, JPEG, and PNG files are allowed'
            }), 400

        # 2. DOCUMENT TYPE KONTROLÃœ
        document_type = request.form.get('document_type')
        
        if not document_type:
            return jsonify({
                'error': 'No document type specified',
                'message': 'Please specify the document_type parameter',
                'available_types': list(DOCUMENT_HANDLERS.keys())
            }), 400

        if document_type not in DOCUMENT_HANDLERS:
            return jsonify({
                'error': 'Invalid document type',
                'message': f'Document type "{document_type}" is not supported',
                'available_types': list(DOCUMENT_HANDLERS.keys())
            }), 400

        # 3. OPSIYONEL PARAMETRELERÄ° AL
        custom_document_url = request.form.get('document_url', None)
        initial_comment = request.form.get('initial_comment', '').strip()
        comment_author = request.form.get('comment_author', '').strip()
        
        logger.info(f"Analiz isteÄŸi (ASYNC) - Tip: {document_type}, Dosya: {file.filename}")

        # 4. DOSYAYI MEMORY'YE AL
        filename = secure_filename(file.filename)
        file_data = file.read()  # DosyayÄ± memory'ye oku
        
        # 5. HANDLER BÄ°LGÄ°SÄ°
        handler_info = DOCUMENT_HANDLERS[document_type]

        # 6. TÄ°CKET OLUÅTUR (Ä°ÅLENÄ°YOR STATUS)
        analysis_id = f"{document_type}_{int(datetime.now().timestamp())}"
        ticket_no = generate_ticket_no()
        
        new_ticket = Ticket(
            ticket_no=ticket_no,
            ticket_id=analysis_id,
            document_name=filename,
            document_type=document_type,
            document_url=custom_document_url if custom_document_url else f"{FILE_BASE_URL}{filename}",
            opening_date=datetime.now(),
            status='Ä°ÅŸleniyor',  # ASYNC: Ä°ÅŸleniyor
            responsible='SavaÅŸ Bey',
            analysis_result={}  # BoÅŸ baÅŸlar
        )
        
        db.session.add(new_ticket)
        db.session.commit()
        
        logger.info(f"Ticket oluÅŸturuldu (ASYNC): {ticket_no} (ID: {analysis_id})")
        
        # 7. ARKA PLANDA ANALÄ°Z BAÅLAT (THREAD)
        thread = threading.Thread(
            target=analyze_document_async,
            args=(file_data, filename, document_type, handler_info, custom_document_url, initial_comment, comment_author, analysis_id)
        )
        thread.daemon = True
        thread.start()
        
        # 8. HEMEN RESPONSE DÃ–N
        return jsonify({
            'success': True,
            'message': 'Analiz baÅŸlatÄ±ldÄ±',
            'ticket_id': analysis_id,
            'ticket_no': ticket_no,
            'status': 'Ä°ÅŸleniyor'
        }), 200

    except Exception as e:
        logger.error(f"Error in analyze_document: {str(e)}")
        return jsonify({
            'error': 'Internal server error',
            'message': str(e)
        }), 500


# ============================================
# SERVICE INFORMATION ENDPOINT
# ============================================
@app.route('/api/services', methods=['GET'])
def get_services():
    """Mevcut tÃ¼m analiz servislerini listele"""
    try:
        services = []
        
        for service_name, config in DOCUMENT_HANDLERS.items():
            services.append({
                'service_name': service_name,
                'description': config['description'],
                'endpoint': config['endpoint'],
                'status': 'available'
            })
        
        return jsonify({
            'success': True,
            'services': services,
            'total_services': len(services)
        })
        
    except Exception as e:
        logger.error(f"Servisler listelenirken hata: {str(e)}")
        return jsonify({
            'success': False,
            'message': str(e)
        }), 500

    
# ============================================
# TICKET MANAGEMENT ENDPOINTS (PostgreSQL)
# ============================================
@app.route('/api/tickets', methods=['GET'])
def get_tickets():
    """TÃ¼m tickets'larÄ± dÃ¶ndÃ¼r (filtreleme ile) - PostgreSQL"""
    try:
        # Base query
        query = Ticket.query
        
        # Filtreleme parametreleri
        ticket_id_filter = request.args.get('ticket_id', '')  # YENÄ°: ticket_id filtresi
        status_filter = request.args.get('status', '')
        date_from = request.args.get('date_from', '')
        date_to = request.args.get('date_to', '')
        responsible_filter = request.args.get('responsible', '')
        
        # Filtrele
        if ticket_id_filter:  # YENÄ°: ticket_id ile filtrele
            query = query.filter(Ticket.ticket_id == ticket_id_filter)
        
        if status_filter:
            query = query.filter(Ticket.status == status_filter)
        
        if date_from:
            date_from_obj = datetime.fromisoformat(date_from)
            query = query.filter(Ticket.opening_date >= date_from_obj)
        
        if date_to:
            date_to_obj = datetime.fromisoformat(date_to).replace(hour=23, minute=59, second=59)
            query = query.filter(Ticket.opening_date <= date_to_obj)
        
        if responsible_filter:
            query = query.filter(Ticket.responsible.ilike(f'%{responsible_filter}%'))
        
        # SÄ±rala (en yeni Ã¶nce)
        tickets = query.order_by(Ticket.opening_date.desc()).all()
        
        # Response formatÄ± (frontend ile uyumlu)
        result = []
        for ticket in tickets:
            # YorumlarÄ± Ã§ek
            comments = TicketComment.query.filter_by(ticket_id=ticket.id).order_by(TicketComment.timestamp).all()
            
            result.append({
                'ticket_no': ticket.ticket_no,
                'ticket_id': ticket.ticket_id,
                'document_name': ticket.document_name,
                'document_type': ticket.document_type,
                'document_url': ticket.document_url,
                'opening_date': ticket.opening_date.isoformat(),
                'last_updated': ticket.last_updated.isoformat() if ticket.last_updated else None,
                'closing_date': ticket.closing_date.isoformat() if ticket.closing_date else None,
                'status': ticket.status,
                'responsible': ticket.responsible,
                'analysis_result': ticket.analysis_result,
                'comments': [
                    {
                        'comment_id': c.comment_id,
                        'author': c.author,
                        'text': c.text,
                        'timestamp': c.timestamp.isoformat()
                    }
                    for c in comments
                ]
            })
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"Tickets okunurken hata: {str(e)}")
        return jsonify([]), 500


@app.route('/api/update-ticket', methods=['POST'])
def update_ticket():
    """Ticket bilgilerini gÃ¼ncelle (status, responsible, vb.) - PostgreSQL"""
    try:
        data = request.get_json()
        
        # HYBRID: ticket_id veya ticket_no kabul et
        ticket_id = data.get('ticket_id')
        ticket_no = data.get('ticket_no')
        
        if not ticket_id and not ticket_no:
            return jsonify({
                'success': False, 
                'message': 'ticket_id veya ticket_no gerekli'
            }), 400
        
        # GÃ¼ncellenebilir alanlar (opsiyonel)
        new_status = data.get('status')
        new_responsible = data.get('responsible')
        
        if not new_status and not new_responsible:
            return jsonify({
                'success': False, 
                'message': 'GÃ¼ncellenecek alan belirtilmedi'
            }), 400
        
        # HYBRID ARAMA: Ã–nce ticket_id, sonra ticket_no
        ticket = None
        if ticket_id:
            ticket = Ticket.query.filter_by(ticket_id=ticket_id).first()
        
        if not ticket and ticket_no:
            ticket = Ticket.query.filter_by(ticket_no=ticket_no).first()
        
        if not ticket:
            return jsonify({'success': False, 'message': 'Ticket bulunamadÄ±'}), 404
        
        old_status = ticket.status
        
        # Status gÃ¼ncelleme
        if new_status:
            ticket.status = new_status
            
            # KapalÄ± statÃ¼sÃ¼ne Ã‡EVRÄ°LDÄ°YSE kapanma tarihini ekle
            if new_status == 'KapalÄ±' and old_status != 'KapalÄ±':
                ticket.closing_date = datetime.now()
            
            # KapalÄ±'dan baÅŸka bir statÃ¼ye GEÃ‡Ä°LDÄ°YSE kapanma tarihini sil
            elif new_status != 'KapalÄ±' and old_status == 'KapalÄ±':
                ticket.closing_date = None
        
        # Sorumlu gÃ¼ncelleme
        if new_responsible:
            ticket.responsible = new_responsible
        
        # Son gÃ¼ncelleme tarihini ekle
        ticket.last_updated = datetime.now()
        
        db.session.commit()
        
        logger.info(f"Ticket gÃ¼ncellendi: {ticket.ticket_no} (ID: {ticket.ticket_id})")
        
        return jsonify({'success': True, 'message': 'Ticket baÅŸarÄ±yla gÃ¼ncellendi'})
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Ticket gÃ¼ncelleme hatasÄ±: {str(e)}")
        return jsonify({'success': False, 'message': str(e)}), 500


@app.route('/api/add-comment', methods=['POST'])
def add_comment():
    """Ticket'a yeni yorum ekle - AI Pattern Update desteÄŸi ile"""
    try:
        data = request.get_json()
        
        ticket_id = data.get('ticket_id')
        ticket_no = data.get('ticket_no')
        author = data.get('author', '').strip()
        text = data.get('text', '').strip()
        
        if not ticket_id and not ticket_no:
            return jsonify({
                'success': False, 
                'message': 'ticket_id veya ticket_no gerekli'
            }), 400
        
        if not author or not text:
            return jsonify({
                'success': False, 
                'message': 'Ä°sim ve yorum boÅŸ olamaz'
            }), 400
        
        # HYBRID ARAMA
        ticket = None
        if ticket_id:
            ticket = Ticket.query.filter_by(ticket_id=ticket_id).first()
        
        if not ticket and ticket_no:
            ticket = Ticket.query.filter_by(ticket_no=ticket_no).first()
        
        if not ticket:
            return jsonify({'success': False, 'message': 'Ticket bulunamadÄ±'}), 404
        
        # Mevcut yorum sayÄ±sÄ±nÄ± bul
        existing_comments_count = TicketComment.query.filter_by(ticket_id=ticket.id).count()
        comment_id = f"comment_{existing_comments_count + 1}"
        
        # Yeni yorum oluÅŸtur
        new_comment = TicketComment(
            ticket_id=ticket.id,
            comment_id=comment_id,
            author=author,
            text=text,
            timestamp=datetime.now()
        )
        
        db.session.add(new_comment)
        
        # Ticket'Ä±n last_updated'ini gÃ¼ncelle
        ticket.last_updated = datetime.now()
        
        # ============================================
        # ğŸ¤– AI PATTERN UPDATE - YETKÄ°LÄ° KULLANICI KONTROLÃœ
        # ============================================
        ai_message = ""
        
        if author in AUTHORIZED_USERS:
            logger.info(f"ğŸ¤– Yetkili kullanÄ±cÄ± yorumu: {author}")

            # Dynamic kontrolÃ¼
            doc_type = DocumentType.query.filter_by(code=ticket.document_type).first()
            if not doc_type or doc_type.service_file != 'dynamic_service.py':
                logger.info(f"â„¹ï¸ Static servis ({ticket.document_type}), AI atlandÄ±")
                db.session.commit()
                return jsonify({"success": True, "message": "Yorum eklendi"})
            
            logger.info(f"ğŸš€ Dynamic servis ({doc_type.name}), AI baÅŸlatÄ±lÄ±yor...")
            
            try:
                # AI ile pattern gÃ¼ncelleme yap
                ai_result = analyze_comment_and_update_patterns(
                    comment=text,
                    ticket=ticket
                )
                
                if ai_result['success']:
                    updated_count = len(ai_result.get('updates', []))
                    if updated_count > 0:
                        ai_message = f" (AI: {updated_count} pattern iyileÅŸtirildi)"
                        logger.info(f"âœ… AI gÃ¼ncelleme baÅŸarÄ±lÄ±: {updated_count} pattern")
                    else:
                        ai_message = " (AI: GÃ¼ncelleme gerekmedi)"
                        logger.info("â„¹ï¸ AI gÃ¼ncelleme gerekmedi")
                else:
                    ai_message = f" (AI hatasÄ±: {ai_result.get('error', 'Bilinmeyen hata')})"
                    logger.error(f"âŒ AI hatasÄ±: {ai_result.get('error')}")
                    
            except Exception as e:
                ai_message = f" (AI hatasÄ±: {str(e)})"
                logger.error(f"âŒ AI gÃ¼ncelleme exception: {str(e)}")
        
        # Database commit
        db.session.commit()
        
        logger.info(f"Yorum eklendi: {ticket.ticket_no} - {author}{ai_message}")
        
        return jsonify({
            'success': True,
            'message': f'Yorum baÅŸarÄ±yla eklendi{ai_message}',
            'comment': {
                'comment_id': comment_id,
                'author': author,
                'text': text,
                'timestamp': new_comment.timestamp.isoformat()
            }
        })
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Yorum ekleme hatasÄ±: {str(e)}")
        return jsonify({'success': False, 'message': str(e)}), 500


@app.route('/api/delete-ticket', methods=['POST'])
def delete_ticket():
    """Ticket'Ä± sil - PostgreSQL (cascade ile yorumlar da silinir)"""
    try:
        data = request.get_json()
        
        ticket_id = data.get('ticket_id')
        ticket_no = data.get('ticket_no')
        
        if not ticket_id and not ticket_no:
            return jsonify({
                'success': False, 
                'message': 'ticket_id veya ticket_no gerekli'
            }), 400
        
        # HYBRID ARAMA
        ticket = None
        if ticket_id:
            ticket = Ticket.query.filter_by(ticket_id=ticket_id).first()
        
        if not ticket and ticket_no:
            ticket = Ticket.query.filter_by(ticket_no=ticket_no).first()
        
        if not ticket:
            return jsonify({'success': False, 'message': 'Ticket bulunamadÄ±'}), 404
        
        ticket_no_deleted = ticket.ticket_no
        
        # Sil (cascade sayesinde yorumlar da silinir)
        db.session.delete(ticket)
        db.session.commit()
        
        logger.info(f"Ticket silindi: {ticket_no_deleted}")
        
        return jsonify({
            'success': True,
            'message': 'Ticket baÅŸarÄ±yla silindi'
        })
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Ticket silme hatasÄ±: {str(e)}")
        return jsonify({'success': False, 'message': str(e)}), 500


# ============================================
# EVALUATION ENDPOINTS
# ============================================
@app.route('/api/save-evaluation', methods=['POST'])
def save_evaluation():
    """Analiz deÄŸerlendirmesini kaydet"""
    try:
        evaluation_data = request.get_json()
        
        if not evaluation_data:
            return jsonify({
                'success': False,
                'message': 'DeÄŸerlendirme verisi bulunamadÄ±'
            }), 400
        
        evaluations_file = 'analysis_evaluations.json'
        
        evaluations = []
        if os.path.exists(evaluations_file):
            try:
                with open(evaluations_file, 'r', encoding='utf-8') as f:
                    evaluations = json.load(f)
            except Exception as e:
                logger.warning(f"Mevcut deÄŸerlendirmeler okunamadÄ±: {e}")
                evaluations = []
        
        evaluations.append(evaluation_data)
        
        with open(evaluations_file, 'w', encoding='utf-8') as f:
            json.dump(evaluations, f, ensure_ascii=False, indent=2)
        
        logger.info(f"DeÄŸerlendirme kaydedildi: {evaluation_data.get('document_name', 'Unknown')}")
        
        return jsonify({
            'success': True,
            'message': 'DeÄŸerlendirme baÅŸarÄ±yla kaydedildi',
            'evaluation_count': len(evaluations)
        })
        
    except Exception as e:
        logger.error(f"DeÄŸerlendirme kaydetme hatasÄ±: {str(e)}")
        return jsonify({
            'success': False,
            'message': f'DeÄŸerlendirme kaydedilemedi: {str(e)}'
        }), 500


@app.route('/api/evaluations', methods=['GET'])
def get_evaluations():
    """TÃ¼m deÄŸerlendirmeleri dÃ¶ndÃ¼r"""
    try:
        evaluations_file = 'analysis_evaluations.json'
        
        if not os.path.exists(evaluations_file):
            return jsonify([])
        
        with open(evaluations_file, 'r', encoding='utf-8') as f:
            evaluations = json.load(f)
        
        evaluations.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
        
        return jsonify(evaluations)
        
    except Exception as e:
        logger.error(f"DeÄŸerlendirmeler okunurken hata: {str(e)}")
        return jsonify([])


# ============================================
# INFO & HEALTH ENDPOINTS
# ============================================
@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'service': 'PILZ Report Checker API Gateway',
        'version': '3.0.0-azure-db',
        'architecture': 'database-driven',
        'available_document_types': list(DOCUMENT_HANDLERS.keys()),
        'total_services': len(DOCUMENT_HANDLERS)
    })


@app.route('/api/info', methods=['GET'])
def api_info():
    """API documentation"""
    return jsonify({
        'service': 'PILZ Report Checker API Gateway',
        'version': '3.0.0-azure-db',
        'description': 'Unified API for document analysis services - PostgreSQL powered',
        'architecture': 'Database-driven (PostgreSQL)',
        'endpoints': {
            'POST /api/analyze': 'Analyze any supported document type',
            'GET /api/tickets': 'List all tickets (with filters)',
            'POST /api/update-ticket': 'Update ticket status/responsible',
            'POST /api/add-comment': 'Add comment to ticket',
            'POST /api/delete-ticket': 'Delete ticket',
            'POST /api/save-evaluation': 'Save analysis evaluation',
            'GET /api/evaluations': 'Get all evaluations',
            'GET /api/health': 'Health check',
            'GET /api/info': 'This information',
            'GET /': 'Web interface'
        },
        'document_types': {
            doc_type: info['description'] 
            for doc_type, info in DOCUMENT_HANDLERS.items()
        },
        'usage': {
            'analyze_endpoint': '/api/analyze',
            'method': 'POST',
            'required_fields': ['file', 'document_type'],
            'optional_fields': ['document_url', 'initial_comment', 'comment_author'],
            'supported_formats': list(ALLOWED_EXTENSIONS),
            'max_file_size': '32MB',
            'example_curl': 'curl -X POST -F "file=@document.pdf" -F "document_type=vibration_report" http://localhost:8000/api/analyze'
        }
    })


@app.route('/', methods=['GET'])
def index():
    """Main page with web interface"""
    return render_template('index.html')


# ============================================
# DOCUMENT TYPES ENDPOINT
# ============================================
@app.route('/api/document-types', methods=['GET'])
def get_document_types():
    """Database'den aktif document type'larÄ± dÃ¶ndÃ¼r"""
    try:
        # Aktif document type'larÄ± Ã§ek
        doc_types = DocumentType.query.filter_by(is_active=True).order_by(DocumentType.name).all()
        
        return jsonify({
            'success': True,
            'document_types': [
                {
                    'code': dt.code,
                    'name': dt.name,
                    'description': dt.description,
                    'icon': dt.icon,
                    'endpoint': dt.endpoint
                }
                for dt in doc_types
            ],
            'total': len(doc_types)
        })
        
    except Exception as e:
        logger.error(f"Document types hatasÄ±: {str(e)}")
        return jsonify({
            'success': False,
            'message': str(e)
        }), 500


# ============================================
# DATABASE INITIALIZATION
# ============================================
# Database'i baÅŸlat
init_db(app)

# ============================================
# DYNAMIC DOCUMENT TYPE MANAGEMENT (YENÄ°)
# ============================================
@app.route('/api/create-dynamic-type', methods=['POST'])
def create_dynamic_type():
    """Yeni dÃ¶kÃ¼man tÃ¼rÃ¼ oluÅŸtur - AI ile pattern generation"""
    try:
        data = request.get_json()
        
        # Validasyon
        if not data.get('name'):
            return jsonify({'success': False, 'message': 'DÃ¶kÃ¼man adÄ± gerekli'}), 400
        
        if not data.get('strong_keywords') or len(data['strong_keywords']) == 0:
            return jsonify({'success': False, 'message': 'En az 1 strong keyword gerekli'}), 400
        
        # Code oluÅŸtur (normalize)
        code = data['name'].lower().strip().replace(' ', '_').replace('ÅŸ', 's').replace('ÄŸ', 'g').replace('Ã¼', 'u').replace('Ã¶', 'o').replace('Ã§', 'c').replace('Ä±', 'i')
        
        # AynÄ± code var mÄ± kontrol
        existing = DocumentType.query.filter_by(code=code).first()
        if existing:
            return jsonify({'success': False, 'message': f'Bu dÃ¶kÃ¼man tÃ¼rÃ¼ zaten var: {code}'}), 400
        
        logger.info(f"ğŸš€ Yeni dÃ¶kÃ¼man tÃ¼rÃ¼ oluÅŸturuluyor: {data['name']} ({code})")
        logger.info(f"ğŸ“Š AI pattern generation baÅŸlatÄ±lÄ±yor...")
        
        # AI'ya gÃ¶nder - Patterns oluÅŸtur
        ai_result = generate_patterns_with_ai(data)
        
        if not ai_result['success']:
            return jsonify({'success': False, 'message': f'AI pattern hatasÄ±: {ai_result["error"]}'}), 500
        
        logger.info(f"âœ… AI patterns oluÅŸturuldu")
        
        # Database'e kaydet
        doc_type = DocumentType(
            code=code,
            name=data['name'],
            description=data.get('description', f"{data['name']} analiz servisi"),
            service_file='dynamic_service.py',
            endpoint='/api/dynamic-report',
            icon=data.get('icon', 'ğŸ“„'),
            app_variable_name='dynamic_app',
            is_active=True,
            needs_ocr=data.get('needs_ocr', True)
        )
        
        db.session.add(doc_type)
        db.session.flush()
        
        # Criteria Weights ekle
        for idx, (category_name, weight) in enumerate(data['criteria_weights'].items(), 1):
            cw = CriteriaWeight(
                document_type_id=doc_type.id,
                category_name=category_name,
                weight=weight,
                display_order=idx
            )
            db.session.add(cw)
            db.session.flush()

            logger.info(f"ğŸ“Š Kategori eklendi: {category_name}") 

            # ğŸ‘‡ CASE-INSENSITIVE EÅLEÅME
            # AI'dan gelen kategori adÄ±nÄ± bul (bÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf fark etmez)
            matched_category = None
            for ai_category in ai_result['criteria_patterns'].keys():
                if ai_category.lower().replace('_', ' ') == category_name.lower().replace('_', ' '):
                    matched_category = ai_category
                    break
            
            # Criteria Details ekle (AI'dan gelen patterns ile)
            if matched_category:
                logger.info(f"   âœ… AI patterns bulundu: {len(ai_result['criteria_patterns'][matched_category])} kriter")
                for idx2, (criterion_name, criterion_data) in enumerate(ai_result['criteria_patterns'][matched_category].items(), 1):
                    logger.info(f"      - {criterion_name}: {criterion_data['weight']} puan")
                    cd = CriteriaDetail(
                        criteria_weight_id=cw.id,
                        criterion_name=criterion_name,
                        pattern=criterion_data['pattern'],
                        weight=criterion_data['weight'],
                        display_order=idx2
                    )
                    db.session.add(cd)
            else:
                logger.warning(f"   âš ï¸  AI patterns BULUNAMADI: {category_name}")  
                logger.warning(f"   AI result keys: {ai_result['criteria_patterns'].keys()}")
                logger.warning(f"   AI keys: {list(ai_result['criteria_patterns'].keys())}")  
                
        # Pattern Definitions ekle (extract_values - AI'dan gelen)
        if ai_result.get('extract_patterns'):
            for idx, (field_name, patterns_list) in enumerate(ai_result['extract_patterns'].items(), 1):
                pd = PatternDefinition(
                    document_type_id=doc_type.id,
                    pattern_group='extract_values',
                    field_name=field_name,
                    patterns=patterns_list,
                    display_order=idx
                )
                db.session.add(pd)
        
        # Validation Keywords - Critical Terms (AI'dan gelen)
        if ai_result.get('critical_terms'):
            for idx, keywords_list in enumerate(ai_result['critical_terms'], 1):
                vk = ValidationKeyword(
                    document_type_id=doc_type.id,
                    keyword_type='critical_terms',
                    category=f'category_{idx}',
                    keywords=keywords_list
                )
                db.session.add(vk)
        
        # Strong Keywords
        vk_strong = ValidationKeyword(
            document_type_id=doc_type.id,
            keyword_type='strong_keywords',
            keywords=data['strong_keywords']
        )
        db.session.add(vk_strong)
        db.session.flush()

        # Excluded Keywords - Mevcut pool'u Ã§ek + yeni strong keywords ekle
        excluded_pool = get_excluded_keywords_pool(current_doc_type_id=doc_type.id) 
        
        
        vk_excluded = ValidationKeyword(
            document_type_id=doc_type.id,
            keyword_type='excluded_keywords',
            keywords=list(set(excluded_pool))  # Duplicate'leri kaldÄ±r
        )
        db.session.add(vk_excluded)
        
        db.session.commit()
        
        logger.info(f"âœ… DÃ¶kÃ¼man tÃ¼rÃ¼ kaydedildi: {code}")
        logger.info(f"ğŸ”„ Document handlers yeniden yÃ¼kleniyor...")
        
        # Handlers'Ä± yeniden yÃ¼kle
        load_document_handlers()
        
        return jsonify({
            'success': True,
            'message': f'âœ… {data["name"]} baÅŸarÄ±yla oluÅŸturuldu!',
            'code': code,
            'document_type': {
                'code': code,
                'name': data['name'],
                'icon': data.get('icon', 'ğŸ“„')
            }
        })
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"âŒ Create dynamic type hatasÄ±: {str(e)}")
        return jsonify({'success': False, 'message': str(e)}), 500

@app.route('/api/delete-dynamic-type', methods=['POST'])
def delete_dynamic_type():
    """Dynamic dÃ¶kÃ¼man tÃ¼rÃ¼nÃ¼ sil - Cascade ile tÃ¼m iliÅŸkili veriler silinir"""
    try:
        data = request.get_json()
        code = data.get('code')
        
        if not code:
            return jsonify({'success': False, 'message': 'DÃ¶kÃ¼man kodu gerekli'}), 400
        
        # DÃ¶kÃ¼man tÃ¼rÃ¼nÃ¼ bul
        doc_type = DocumentType.query.filter_by(code=code).first()
        
        if not doc_type:
            return jsonify({'success': False, 'message': f'DÃ¶kÃ¼man tÃ¼rÃ¼ bulunamadÄ±: {code}'}), 404
        
        # Dynamic service mi kontrol et
        if doc_type.service_file != 'dynamic_service.py':
            return jsonify({'success': False, 'message': 'Sadece dynamic dÃ¶kÃ¼man tÃ¼rleri silinebilir'}), 400
        
        doc_name = doc_type.name
        
        logger.info(f"ğŸ—‘ï¸ DÃ¶kÃ¼man tÃ¼rÃ¼ siliniyor: {doc_name} ({code})")
        
        # Strong keywords'leri excluded pool'dan Ã§Ä±kar
        try:
            strong_vk = ValidationKeyword.query.filter_by(
                document_type_id=doc_type.id,
                keyword_type='strong_keywords'
            ).first()
            
            if strong_vk:
                logger.info(f"   ğŸ“¤ Strong keywords excluded pool'dan Ã§Ä±karÄ±lÄ±yor: {strong_vk.keywords}")
        except Exception as e:
            logger.warning(f"Strong keywords Ã§Ä±karma hatasÄ±: {e}")
        
        # Cascade delete (iliÅŸkili tÃ¼m kayÄ±tlar silinir)
        db.session.delete(doc_type)
        db.session.commit()
        
        logger.info(f"âœ… {doc_name} baÅŸarÄ±yla silindi")
        logger.info(f"ğŸ”„ Document handlers yeniden yÃ¼kleniyor...")
        
        # Handlers'Ä± yeniden yÃ¼kle
        load_document_handlers()
        
        return jsonify({
            'success': True,
            'message': f'âœ… {doc_name} baÅŸarÄ±yla silindi!',
            'code': code
        })
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"âŒ Delete dynamic type hatasÄ±: {str(e)}")
        return jsonify({'success': False, 'message': str(e)}), 500


@app.route('/api/get-dynamic-types', methods=['GET'])
def get_dynamic_types():
    """TÃ¼m dynamic dÃ¶kÃ¼man tÃ¼rlerini listele"""
    try:
        # is_dynamic flag'i yoksa service_file='dynamic_service.py' olanlarÄ± al
        dynamic_types = DocumentType.query.filter_by(
            service_file='dynamic_service.py',
            is_active=True
        ).order_by(DocumentType.name).all()
        
        return jsonify({
            'success': True,
            'types': [
                {
                    'code': dt.code,
                    'name': dt.name,
                    'icon': dt.icon,
                    'description': dt.description
                }
                for dt in dynamic_types
            ]
        })
        
    except Exception as e:
        logger.error(f"Get dynamic types hatasÄ±: {str(e)}")
        return jsonify({'success': False, 'message': str(e)}), 500


# ============================================
# HELPER FUNCTIONS - AI & KEYWORDS
# ============================================
def generate_patterns_with_ai(data):
    try:
        import anthropic
        import os
        import json
        import re

        # API key kontrolÃ¼
        api_key = os.getenv('ANTHROPIC_API_KEY')
        if not api_key:
            logger.error("âŒ ANTHROPIC_API_KEY bulunamadÄ±!")
            return {
                "success": False,
                "error": "API key tanÄ±mlanmamÄ±ÅŸ. LÃ¼tfen sistem yÃ¶neticisiyle iletiÅŸime geÃ§in."
            }
        
        client = anthropic.Anthropic(api_key=api_key)
        
        # Prompt hazÄ±rla
        prompt = f"""Sen bir regex pattern uzmanÄ±sÄ±n. TÃ¼rkÃ§e ve Ä°ngilizce dÃ¶kÃ¼manlardan bilgi Ã§Ä±karmak iÃ§in regex pattern'leri oluÅŸturuyorsun.

GÃ–REV 1 - CRITERIA DETAILS PATTERNS:

Kategoriler ve Kelimeler:
{format_criteria_for_ai(data['criteria_weights'], data['criteria_details'])}

PATTERN KURALLARI:
1. Her kelime iÃ§in TEK bir UZUN regex pattern oluÅŸtur
2. (?i) SADECE pattern'in EN BAÅINDA kullan! Pattern ORTASINDA veya SONUNDA (?i) ASLA kullanma!
3. Non-capturing group kullan: (?:...)
4. TÃ¼rkÃ§e ve Ä°ngilizce alternatifleri iÃ§ermeli
5. BoÅŸluk toleranslÄ±: \\s* veya \\s+

Ã–RNEK UZUN PATTERN:
"(?i)(?:test\\s*yapan|tested\\s*by|Ã¶lÃ§Ã¼m\\s*yapan|measured\\s*by|kurum|institution|organization|ÅŸirket|company|firma|sorumlu|responsible)[\\s\\W]*[:=]?\\s*([A-ZÃ‡ÄÄ°Ã–ÅÃœa-zÃ¼Ã§ÄŸÄ±Ã¶ÅŸÃ¼][A-Za-zÃ¼Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄÄ°Ã–ÅÃœ\\s\\.\\&\\-]{{3,50}})"

DOÄRU YAZIM:
- "(?i)(?:yazar|author|writer)"
- "(?i)(?:rapor\\s*no|report\\s*number)\\s*[:=]?\\s*([A-Z0-9-]+)"

YANLIÅ YAZIM (YAPMA!):
- "(?i)yazar|author" âŒ (non-capturing group yok)
- "yazar|author" âŒ (case flag yok)

WEIGHT HESAPLAMA KURALLARI:  
Her kategori iÃ§in weight'leri eÅŸit daÄŸÄ±t:
- weight = kategori_toplam_puanÄ± / kelime_sayÄ±sÄ±
- Tam bÃ¶lÃ¼nmÃ¼yorsa kalanÄ± son kelimeye ekle

Ã–rnek:
Kategori: "genel bilgiler" (100 puan)
Kelimeler: ["firma_adi", "rapor_no", "tarih"] (3 kelime)
Hesaplama: 100 / 3 = 33.33
Weight daÄŸÄ±lÄ±mÄ±:
- firma_adi: 33
- rapor_no: 33
- tarih: 34 (kalan +1 puan)

Ã–rnek 2:
Kategori: "teknik bilgiler" (50 puan)
Kelimeler: ["voltaj", "akim"] (2 kelime)
Hesaplama: 50 / 2 = 25
Weight daÄŸÄ±lÄ±mÄ±:
- voltaj: 25
- akim: 25

GÃ–REV 2 - EXTRACT VALUES PATTERNS:

Alanlar:
{format_extract_values_for_ai(data.get('extract_values', []))}

PATTERN KURALLARI:
Her alan iÃ§in 2-4 FARKLI pattern oluÅŸtur:

KRÄ°TÄ°K UYARI - (?i) KULLANIMI:
- (?i) SADECE pattern'in EN BAÅINDA kullan!
- Pattern ORTASINDA veya SONUNDA (?i) ASLA kullanma!

1. ALAN ADINI GENÄ°ÅLET:
   - TÃ¼rkÃ§e + Ä°ngilizce alternatifleri
   - YaygÄ±n varyasyonlarÄ± ekle

2. BOÅLUK TOLERANSsI:
   - \\s* veya \\s+ veya [\\s\\W]* kullan
   - Ä°ki nokta/eÅŸittir opsiyonel: [:=]?

3. DEÄER YAKALAMA (alan tÃ¼rÃ¼ne gÃ¶re):
   - TARÄ°H: (\\d{{1,2}}[./\\-]\\d{{1,2}}[./\\-]\\d{{2,4}}) veya (\\d{{1,2}}\\s+\\d{{1,2}}\\s+\\d{{2,4}})
   - NUMARA/KOD: ([A-Z0-9\\-/]+) veya ([0-9]{{3,}})
   - Ä°SÄ°M/FÄ°RMA: ([A-ZÃ‡ÄÄ°Ã–ÅÃœ][A-Za-zÃ¼Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄÄ°Ã–ÅÃœ\\s\\.\\&\\-]{{3,80}})
   - GENEL METÄ°N: ([^\\n]{{5,100}})

4. FARKLI FORMAT VARYASYONLARI:
   - Alan adÄ± Ã–NCE, sonra deÄŸer: "(?i)(?:alan_adi)[\\s\\W]*[:=]?\\s*(deÄŸer_pattern)"
   - DeÄŸer Ã–NCE, sonra alan adÄ±: "(deÄŸer_pattern)\\s*(?:alan_adi)"
   - BoÅŸluklu format: "(?i)(?:alan_adi)\\s+(deÄŸer_pattern)"

5. HER PATTERN:
   - (?i) ile baÅŸla (case-insensitive)
   - Non-capturing group: (?:...)
   - Capture group: (...) sadece deÄŸer iÃ§in

Ã–RNEK ARRAY PATTERN:
[
    "(?i)(?:test\\s*tarih|test\\s*date)\\s*[:=]\\s*(\\d{{1,2}}[./]\\d{{1,2}}[./]\\d{{2,4}})",
    "(\\d{{1,2}}[./]\\d{{1,2}}[./]\\d{{4}})\\s*(?:tarih|date)"
]

GÃ–REV 3 - CRITICAL TERMS:

Strong Keywords: {', '.join(data['strong_keywords'])}

Kurallar:
- Mevcut kelimeleri kullan
- TÃ¼rkÃ§e/Ä°ngilizce alternatifleri ekle  
- Kategoriler mantÄ±klÄ± olmalÄ±
- 3-4 kategori oluÅŸtur

Ã–rnek:
Input: ["termal", "konfor"]
Output: 
[
    ["termal", "thermal", "Ä±sÄ±", "heat", "sÄ±caklÄ±k", "temperature"],
    ["konfor", "comfort", "rahatlÄ±k"]
]

KRÄ°TÄ°K JSON KURALLARI:
1. Regex pattern'lerinde backslash DÃ–RT KERE: \\\\s \\\\d \\\\w \\\\S
2. String iÃ§inde Ã§ift tÄ±rnak varsa escape et: \\"
3. Yeni satÄ±r kullanma, tek satÄ±r JSON dÃ¶ndÃ¼r
4. TÃ¼rkÃ§e karakterleri olduÄŸu gibi bÄ±rak (escape etme)

JSON formatÄ±nda dÃ¶ndÃ¼r - Ã–NEMLÄ°: Regex pattern'lerinde backslash'leri DÃ–RT KERE yaz (\\\\s \\\\d \\\\w):
{{
    "criteria_patterns": {{
        "kategori_ismi": {{
            "kelime_adi": {{
                "pattern": "(?i)(?:pattern_buraya)",
                "weight": 33 // Ã–RNEK! Sen hesapla!
            }}
        }}
    }},
    "extract_patterns": {{
        "field_name": ["(?i)pattern1","(?i)pattern2"]
    }},
    "critical_terms": [
        ["kelime1", "kelime2"],
        ["kelime3", "kelime4"]
    ]
}}

Ã–NEMLÄ° UYARILAR:
1. Kategori isimleri AYNEN ÅŸunlar olmalÄ±: {list(data['criteria_weights'].keys())}
2. Weight'leri yukarÄ±daki "WEIGHT HESAPLAMA KURALLARI"na gÃ¶re MUTLAKA HESAPLA!
3. JSON Ã¶rneÄŸindeki "33" sadece Ã¶rnek, gerÃ§ek deÄŸerleri sen hesapla!  
4. JSON iÃ§inde backslash dÃ¶rt kere: \\\\s \\\\d \\\\w
"""
        
        message = client.messages.create(
            model="claude-sonnet-4-5-20250929",
            max_tokens=3000,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        # Response'u parse et
        response_text = message.content[0].text
        
        # JSON'u Ã§Ä±kar (markdown varsa temizle)
        import json
        if '```json' in response_text:
            response_text = response_text.split('```json')[1].split('```')[0]
        elif '```' in response_text:
            response_text = response_text.split('```')[1].split('```')[0]

        # Fazla boÅŸluklarÄ± temizle
        response_text = response_text.strip()

        # ğŸ‘‡ YENÄ°: Backslash kontrolÃ¼
        logger.info("ğŸ“ AI Response ilk 500 karakter:")
        logger.info(response_text[:500])  
        
        # JSON parse dene
        try:
            result = json.loads(response_text)
        except json.JSONDecodeError as je:
            logger.error(f"âŒ JSON Parse HatasÄ±: {je}")
            logger.error(f"HatalÄ± JSON (ilk 1000 karakter):")
            logger.error(response_text[:1000])
            
            # ğŸ‘‡ YENÄ°: Otomatik dÃ¼zeltme dene
            logger.info("ğŸ”§ JSON dÃ¼zeltme deneniyor...")
            
            # Tek backslash'leri double yap (sadece regex pattern'lerinde)
            fixed_text = re.sub(
                r'("pattern":\s*"[^"]*)(\\)([^\\"])',
                r'\1\\\\\3',
                response_text
            )
            
            try:
                result = json.loads(fixed_text)
                logger.info("âœ… JSON otomatik dÃ¼zeltildi!")
            except:
                return {
                    'success': False, 
                    'error': f'AI JSON parse hatasÄ±: {str(je)}\n\nÄ°lk 500 karakter:\n{response_text[:500]}'
                }
        
        result['success'] = True
        return result
        
    except Exception as e:
        logger.error(f"AI pattern generation hatasÄ±: {str(e)}")
        return {'success': False, 'error': str(e)}


def format_criteria_for_ai(criteria_weights, criteria_details):
    """Criteria'larÄ± AI iÃ§in formatla"""
    text = ""
    for category, weight in criteria_weights.items():
        text += f"\n{category} (Puan: {weight}):\n"
        if category in criteria_details:
            for keyword in criteria_details[category]:
                text += f"  - {keyword}\n"
    return text


def format_extract_values_for_ai(extract_values):
    """Extract values'larÄ± AI iÃ§in formatla"""
    if not extract_values:
        return "Yok"
    return "\n".join([f"  - {field}" for field in extract_values])


def get_excluded_keywords_pool(current_doc_type_id=None):
    """Mevcut excluded keywords pool'unu dÃ¶ndÃ¼r"""
    base_pool = [
        "hrc","cobot","robot","Ã§arpÄ±ÅŸma","collaborative","kolaboratif","sd conta",
        "elektrik", "devre", "ÅŸema", "circuit", "electrical", "voltage", "amper", "ohm", "enclosure", "wrp-", "light curtain", "contactors", "controller",
        "espe",
        "hidrolik", "HÄ°DROLÄ°K", "hydraulic", "hidrolik yaÄŸ", "hydraulic oil", "iso 1219", "1219", "teknik resim",
        "gÃ¼rÃ¼ltÃ¼", "noise", "ses", "sound", "decibel", "db", "akustik", "acoustic",
        "kullanma", "kÄ±lavuz", "manual", "instruction", "talimat", "guide", "kÄ±lavuzu",
        "loto",
        "lvd", "TOPRAKLAMA SÃœREKLÄ°LÄ°K", "topraklama sÃ¼reklilik", "TOPRAKLAMA Ä°LETKENLERÄ°", "topraklama iletkenleri",
        "uygunluk", "beyan", "muayene", "conformity", "declaration", "declare",
        "isg", "periyodik", "kontrol", "periodic", "inspection", "denetim",
        "pnÃ¶matik", "pnomatik", "pneumatic", "lubricator", "inflate", "psi", "bar", "regis", "r102", "regulator", "dump valve", "oil",
        "montaj", "assembly",
        "topraklama direnci", "grounding", "earthing", "60204", "topraklama", "TOPRAKLAMA DÄ°RENCÄ°",
        "bakÄ±m", "maintenance", "servis", "service", "bakim", "MAINTENCE",
        "titreÅŸim", "vibration", "mekanik",
        "aydÄ±nlatma", "lighting", "illumination", "lux", "lÃ¼men", "lumen", "ts en 12464", "en 12464", "Ä±ÅŸÄ±k", "Ä±ÅŸÄ±k ÅŸiddeti",
        "AT TÄ°P", "at tip", "ec type", "SERTÄ°FÄ°KA", "sertifika", "certificate",
    ]

    logger.info(f"ğŸ“¦ Base pool: {len(base_pool)} kelime")

    # ============================================
    # 2. DÄ°ÄER DYNAMIC SERVÄ°SLERÄ°N STRONG KEYWORDS'LERÄ°
    # ============================================
    try:
        # SADECE strong_keywords (critical_terms DEÄÄ°L!)
        query = db.session.query(ValidationKeyword).join(DocumentType, ValidationKeyword.document_type_id == DocumentType.id).filter(ValidationKeyword.keyword_type == 'strong_keywords',DocumentType.service_file == 'dynamic_service.py')
        # Kendisini hariÃ§ tut
        if current_doc_type_id:
            query = query.filter(ValidationKeyword.document_type_id != current_doc_type_id)
        
        other_strong = query.all()
        
        # DiÄŸer servislerin strong keywords'lerini ekle
        for vk in other_strong:
            base_pool.extend([kw.lower().strip() for kw in vk.keywords])
        
        logger.info(f"â• Dynamic servisler: {len(other_strong)} servis, strong keywords eklendi")
        
    except Exception as e:
        logger.error(f"âŒ Dynamic keywords hatasÄ±: {e}")
    
    # ============================================
    # 3. TEMÄ°ZLEME
    # ============================================
    # Unique + lowercase + boÅŸluksuz
    final_pool = list(set([kw.lower().strip() for kw in base_pool if kw.strip()]))
    
    logger.info(f"âœ… FINAL excluded pool: {len(final_pool)} unique kelime")
    
    return final_pool

# ============================================
# AI-POWERED PATTERN UPDATE FUNCTIONS
# ============================================
def analyze_comment_and_update_patterns(comment: str, ticket: Ticket) -> Dict[str, Any]:
    """
    KullanÄ±cÄ± yorumunu AI ile analiz et ve pattern'leri gÃ¼ncelle
    
    Args:
        comment: KullanÄ±cÄ±nÄ±n yorumu
        ticket: Ticket objesi
        
    Returns:
        {
            'success': bool,
            'updates': [...],
            'error': str (optional)
        }
    """
    try:
        # 1. Document type'Ä± al
        doc_type = DocumentType.query.filter_by(
            code=ticket.document_type,
            is_active=True
        ).first()
        
        if not doc_type:
            return {'success': False, 'error': f'Document type bulunamadÄ±: {ticket.document_type}'}
        
        logger.info(f"ğŸ“‹ Document type: {doc_type.name} ({doc_type.code})")
        
        # 2. Mevcut config'i topla
        current_config = collect_current_patterns(doc_type)
        
        # 3. Analiz sonuÃ§larÄ±nÄ± formatla
        analysis_summary = format_analysis_results(ticket)
        
        # 4. AI'ya gÃ¶nder
        logger.info("ğŸ¤– AI'ya istek gÃ¶nderiliyor...")
        ai_response = call_ai_for_pattern_update(
            comment=comment,
            doc_type_name=doc_type.name,
            current_config=current_config,
            analysis_summary=analysis_summary
        )
        
        if not ai_response['success']:
            return ai_response
        
        # 5. AI'dan gelen gÃ¼ncellemeleri uygula
        updates = ai_response.get('updates', [])
        
        if not updates or len(updates) == 0:
            logger.info("â„¹ï¸ AI gÃ¼ncelleme Ã¶nermedi")
            return {'success': True, 'updates': []}
        
        logger.info(f"ğŸ“ {len(updates)} gÃ¼ncelleme uygulanacak...")
        
        applied_updates = []
        for update in updates:
            try:
                apply_pattern_update(doc_type, update)
                applied_updates.append(update)
                logger.info(f"âœ… GÃ¼ncellendi: {update.get('field_name', 'N/A')}")
            except Exception as e:
                logger.error(f"âŒ GÃ¼ncelleme hatasÄ±: {e}")
        
        db.session.commit()
        
        return {
            'success': True,
            'updates': applied_updates
        }
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"âŒ AI pattern update hatasÄ±: {str(e)}")
        return {'success': False, 'error': str(e)}


def collect_current_patterns(doc_type: DocumentType) -> Dict[str, Any]:
    """Mevcut pattern'leri topla"""
    config = {
        'extract_values': {},
        'criteria_details': {}
    }
    
    # Extract values patterns
    extract_patterns = PatternDefinition.query.filter_by(
        document_type_id=doc_type.id,
        pattern_group='extract_values'
    ).all()
    
    for pattern in extract_patterns:
        config['extract_values'][pattern.field_name] = {
            'patterns': pattern.patterns,
            'id': pattern.id
        }
    
    # Criteria details patterns
    criteria_weights = CriteriaWeight.query.filter_by(
        document_type_id=doc_type.id
    ).all()
    
    for cw in criteria_weights:
        config['criteria_details'][cw.category_name] = {}
        
        criteria_details = CriteriaDetail.query.filter_by(
            criteria_weight_id=cw.id
        ).all()
        
        for cd in criteria_details:
            config['criteria_details'][cw.category_name][cd.criterion_name] = {
                'pattern': cd.pattern,
                'weight': cd.weight,
                'id': cd.id
            }
    
    return config


def format_analysis_results(ticket: Ticket) -> Dict[str, Any]:
    """Analiz sonuÃ§larÄ±nÄ± AI iÃ§in formatla"""
    analysis_result = ticket.analysis_result or {}
    
    summary = {
        'extracted_values': {},
        'category_scores': {}
    }
    
    # Extracted values (hangileri bulunamadÄ±)
    extracted = analysis_result.get('extracted_values', {})
    for field, value in extracted.items():
        status = 'found' if value and value not in ['BulunamadÄ±', 'N/A', ''] else 'not_found'
        summary['extracted_values'][field] = {
            'value': value,
            'status': status
        }
    
    # Category scores (hangi kriterler baÅŸarÄ±sÄ±z/zayÄ±f)
    category_scores = analysis_result.get('category_scores', {})
    
    # âš ï¸ SORUN: category_scores formatÄ± bilinmiyor, dinamik parse gerekli
    # Frontend'den gelen format: {"Genel Bilgiler": {"percentage": 45, ...}}
    # Ama kriter bazlÄ± detay yok! 
    
    # GeÃ§ici Ã§Ã¶zÃ¼m: Sadece kategori seviyesinde bilgi ver
    for category, score_data in category_scores.items():
        if isinstance(score_data, dict):
            percentage = score_data.get('percentage', 0)
            summary['category_scores'][category] = {
                'percentage': percentage,
                'status': 'good' if percentage >= 70 else 'needs_improvement' if percentage >= 40 else 'failed'
            }
    
    return summary


def call_ai_for_pattern_update(comment: str, doc_type_name: str, current_config: Dict, analysis_summary: Dict) -> Dict[str, Any]:
    """AI'ya pattern gÃ¼ncelleme isteÄŸi gÃ¶nder"""
    try:
        api_key = os.getenv('ANTHROPIC_API_KEY')
        if not api_key:
            return {'success': False, 'error': 'ANTHROPIC_API_KEY tanÄ±mlÄ± deÄŸil'}
        
        client = anthropic.Anthropic(api_key=api_key)
        
        # Prompt oluÅŸtur
        prompt = f"""Sen bir regex pattern uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n yorumuna gÃ¶re dÃ¶kÃ¼man analiz pattern'lerini iyileÅŸtiriyorsun.

DÃ–KÃœMAN TÃœRÃœ: {doc_type_name}

KULLANICI YORUMU:
"{comment}"

MEVCUT PATTERN'LER:
{json.dumps(current_config, indent=2, ensure_ascii=False)}

ANALÄ°Z DURUMU:
{json.dumps(analysis_summary, indent=2, ensure_ascii=False)}

GÃ–REV:
KullanÄ±cÄ±nÄ±n yorumunu analiz et ve SADECE ilgili pattern'leri iyileÅŸtir.

Ã–NEMLÄ° KURALLAR:
1. SADECE kullanÄ±cÄ±nÄ±n bahsettiÄŸi field'larÄ± gÃ¼ncelle
2. "Firma adÄ± bulunamadÄ±" â†’ SADECE "firma_adi" pattern'ini gÃ¼Ã§lendir
3. "Kategori X zayÄ±f" â†’ O kategoride SADECE baÅŸarÄ±sÄ±z/geliÅŸtirmeli kriterleri gÃ¼ncelle
4. BaÅŸarÄ±lÄ± kriterleri DOKUNMA (percentage >= 70)
5. Regex'te backslash DÃ–RT KERE: \\\\s \\\\d \\\\w
6. (?i) SADECE pattern'in EN BAÅINDA

PATTERN GÃœÃ‡LENDIRME TAKTÄ°KLERÄ°:
- TÃ¼rkÃ§e + Ä°ngilizce alternatifleri ekle
- BoÅŸluk toleransÄ± artÄ±r: \\\\s* veya [\\\\s\\\\W]*
- Alternatif yazÄ±mlar ekle: (?:tarih|date|tarihi)
- Yakalama grubunu geniÅŸlet

Ã‡IKTI FORMATI (JSON):
{{
    "updates": [
        {{
            "field_type": "extract_values",
            "field_name": "rapor_tarihi",
            "category": null,
            "new_pattern": "(?i)(?:rapor\\\\s*tarih|test\\\\s*date)[\\\\s\\\\W]*[:=]?\\\\s*(\\\\d{{1,2}}[./]\\\\d{{1,2}}[./]\\\\d{{2,4}})",
            "reason": "KullanÄ±cÄ± 'tarih bulunamadÄ±' dedi, pattern'e 'test date' alternatifi eklendi"
        }},
        {{
            "field_type": "criteria_details",
            "field_name": "firma_adi",
            "category": "Genel Bilgiler",
            "new_pattern": "(?i)(?:firma|company|ÅŸirket|organization)[\\\\s\\\\W]*[:=]?\\\\s*([A-ZÃ‡ÄÄ°Ã–ÅÃœ][A-Za-zÃ¼Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄÄ°Ã–ÅÃœ\\\\s\\\\.\\\\&\\\\-]{{3,80}})",
            "reason": "Kategori zayÄ±f, 'organization' alternatifi eklendi"
        }}
    ]
}}

Ã–NEMLÄ°:
- EÄŸer yorum pattern ile ilgili DEÄÄ°LSE â†’ "updates": [] dÃ¶ndÃ¼r
- Genel ÅŸikayet ise (Ã¶r: "kÃ¶tÃ¼ analiz") â†’ "updates": [] dÃ¶ndÃ¼r
- SADECE spesifik field/kategori bahsedilirse gÃ¼ncelle
"""
        
        # AI'ya gÃ¶nder
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=3000,
            messages=[{"role": "user", "content": prompt}]
        )
        
        response_text = message.content[0].text.strip()
        
        # JSON parse
        if '```json' in response_text:
            response_text = response_text.split('```json')[1].split('```')[0]
        elif '```' in response_text:
            response_text = response_text.split('```')[1].split('```')[0]
        
        response_text = response_text.strip()
        
        try:
            result = json.loads(response_text)
        except json.JSONDecodeError as je:
            logger.error(f"âŒ AI JSON parse hatasÄ±: {je}")
            logger.error(f"Response: {response_text[:500]}")
            return {'success': False, 'error': f'AI response JSON hatasÄ±: {str(je)}'}
        
        return {
            'success': True,
            'updates': result.get('updates', [])
        }
        
    except Exception as e:
        logger.error(f"âŒ AI call hatasÄ±: {str(e)}")
        return {'success': False, 'error': str(e)}


def apply_pattern_update(doc_type: DocumentType, update: Dict[str, Any]):
    """Pattern gÃ¼ncelleme uygula"""
    field_type = update.get('field_type')
    field_name = update.get('field_name')
    new_pattern = update.get('new_pattern')
    category = update.get('category')
    
    if not field_type or not field_name or not new_pattern:
        raise ValueError("Eksik update bilgisi")
    
    if field_type == 'extract_values':
        # Extract values pattern gÃ¼ncelle
        pattern_def = PatternDefinition.query.filter_by(
            document_type_id=doc_type.id,
            pattern_group='extract_values',
            field_name=field_name
        ).first()
        
        if pattern_def:
            # Mevcut pattern'lere yeni pattern ekle (baÅŸa ekle - Ã¶ncelikli)
            if new_pattern not in pattern_def.patterns:
                pattern_def.patterns.insert(0, new_pattern)
                logger.info(f"  âœï¸ Extract value gÃ¼ncellendi: {field_name}")
        else:
            logger.warning(f"  âš ï¸ Extract value bulunamadÄ±: {field_name}")
    
    elif field_type == 'criteria_details':
        # Criteria details pattern gÃ¼ncelle
        if not category:
            raise ValueError("Criteria update iÃ§in category gerekli")
        
        # Kategoriyi bul
        cw = CriteriaWeight.query.filter_by(
            document_type_id=doc_type.id,
            category_name=category
        ).first()
        
        if not cw:
            raise ValueError(f"Kategori bulunamadÄ±: {category}")
        
        # Kriteri bul
        cd = CriteriaDetail.query.filter_by(
            criteria_weight_id=cw.id,
            criterion_name=field_name
        ).first()
        
        if cd:
            cd.pattern = new_pattern
            logger.info(f"  âœï¸ Criteria detail gÃ¼ncellendi: {category} â†’ {field_name}")
        else:
            logger.warning(f"  âš ï¸ Criteria detail bulunamadÄ±: {category} â†’ {field_name}")
    
    else:
        raise ValueError(f"Bilinmeyen field_type: {field_type}")


with app.app_context():
    load_document_handlers()
    logger.info(f"âœ… Gunicorn iÃ§in {len(DOCUMENT_HANDLERS)} handler yÃ¼klendi")


# ============================================
# APPLICATION ENTRY POINT (Azure-Friendly)
# ============================================
if __name__ == '__main__':
    logger.info("=" * 60)
    logger.info("PILZ Report Checker - Main API Gateway (PostgreSQL)")
    logger.info("=" * 60)

    logger.info(f"ğŸ”§ Mimari: Database-driven (PostgreSQL)")
    logger.info(f"ğŸ“ Upload klasÃ¶rÃ¼: {UPLOAD_FOLDER}")
    logger.info(f"ğŸ“Š Desteklenen formatlar: {', '.join(ALLOWED_EXTENSIONS)}")
    logger.info(f"ğŸ“ Maksimum dosya boyutu: {app.config['MAX_CONTENT_LENGTH'] // (1024 * 1024)} MB")
    logger.info("")
    logger.info(f"ğŸ“‹ Aktif Servisler ({len(DOCUMENT_HANDLERS)}):")
    for doc_type, info in DOCUMENT_HANDLERS.items():
        logger.info(f"  - {doc_type}: {info['description']}")
    logger.info("=" * 60)
    
    # Azure App Service PORT environment variable
    port = int(os.environ.get('PORT', 8000))
    
    logger.info(f"ğŸš€ Gateway baÅŸlatÄ±lÄ±yor - Port: {port}")
    logger.info("=" * 60)
    
    app.run(
        host='0.0.0.0',
        port=port,
        debug=False
    )