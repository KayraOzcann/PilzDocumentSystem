# ============================================
# DYNAMIC DOCUMENT TYPE ANALYZER
# T√ºm yeni d√∂k√ºman t√ºrlerini y√∂neten tek servis
# Database-driven configuration
# Port: 8018
# ============================================

# ============================================
# IMPORTS
# ============================================
import os
import re
from datetime import datetime
from typing import Dict, List, Any
import PyPDF2
from docx import Document
from dataclasses import dataclass
import logging
import pytesseract
import cv2
import numpy as np
from PIL import Image
import pdf2image

from flask import Flask, request, jsonify
from werkzeug.utils import secure_filename

# ============================================
# DATABASE IMPORTS
# ============================================
from database import db, init_db
from models import DocumentType, CriteriaWeight, CriteriaDetail, PatternDefinition, ValidationKeyword
from config import Config

# ============================================
# LOGGING CONFIGURATION
# ============================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ============================================
# LANGUAGE DETECTION
# ============================================
try:
    from langdetect import detect
    LANGUAGE_DETECTION_AVAILABLE = True
except ImportError:
    LANGUAGE_DETECTION_AVAILABLE = False
    logger.warning("langdetect mod√ºl√º bulunamadƒ± - dil tespiti devre dƒ±≈üƒ±")

# ============================================
# ANALƒ∞Z SINIFI - DATA CLASS
# ============================================
@dataclass
class DynamicAnalysisResult:
    """Dynamic analiz sonucu veri sƒ±nƒ±fƒ±"""
    criteria_name: str
    found: bool
    content: str
    score: int
    max_score: int
    details: Dict[str, Any]

# ============================================
# ANALƒ∞Z SINIFI - MAIN ANALYZER
# ============================================
class DynamicReportAnalyzer:
    """Dynamic d√∂k√ºman t√ºrleri i√ßin analiz sƒ±nƒ±fƒ±"""
    
    def __init__(self, document_code: str, app=None):
        """
        Args:
            document_code: D√∂k√ºman t√ºr√º kodu (√∂rn: 'yangin_sondurme')
            app: Flask app instance
        """
        self.document_code = document_code
        logger.info(f"Dynamic analyzer ba≈ülatƒ±lƒ±yor: {document_code}")
        
        if app:
            with app.app_context():
                try:
                    # Database'den d√∂k√ºman t√ºr√ºn√º bul
                    self.doc_type = DocumentType.query.filter_by(code=document_code, is_active=True).first()
                    
                    if not self.doc_type:
                        raise ValueError(f"D√∂k√ºman t√ºr√º bulunamadƒ±: {document_code}")
                    
                    # Config'i y√ºkle
                    self.load_config()
                    
                    logger.info(f"‚úÖ {self.doc_type.name} config y√ºklendi")
                    logger.info(f"   - OCR: {'Evet' if self.use_ocr else 'Hayƒ±r'}")
                    logger.info(f"   - Kategoriler: {len(self.criteria_weights)}")
                    
                except Exception as e:
                    logger.error(f"‚ö†Ô∏è Config y√ºkleme hatasƒ±: {e}")
                    raise
        else:
            raise ValueError("Flask app context gerekli")
    
    def load_config(self):
        """Database'den config y√ºkle"""
        # Criteria Weights
        self.criteria_weights = {}
        criteria_weights = CriteriaWeight.query.filter_by(
            document_type_id=self.doc_type.id
        ).order_by(CriteriaWeight.display_order).all()
        
        for cw in criteria_weights:
            self.criteria_weights[cw.category_name] = cw.weight
        
        # Criteria Details
        self.criteria_details = {}
        for cw in criteria_weights:
            details = CriteriaDetail.query.filter_by(
                criteria_weight_id=cw.id
            ).order_by(CriteriaDetail.display_order).all()
            
            self.criteria_details[cw.category_name] = {
                cd.criterion_name: {
                    "pattern": cd.pattern,
                    "weight": cd.weight
                }
                for cd in details
            }
        
        # Pattern Definitions (extract_specific_values)
        self.pattern_definitions = {}
        patterns = PatternDefinition.query.filter_by(
            document_type_id=self.doc_type.id
        ).all()
        
        for pattern in patterns:
            if pattern.pattern_group not in self.pattern_definitions:
                self.pattern_definitions[pattern.pattern_group] = {}
            self.pattern_definitions[pattern.pattern_group][pattern.field_name] = pattern.patterns
        
        # Validation Keywords
        self.validation_keywords = {}
        
        # Critical terms
        critical_terms = ValidationKeyword.query.filter_by(
            document_type_id=self.doc_type.id,
            keyword_type='critical_terms'
        ).all()
        self.validation_keywords['critical_terms'] = [vk.keywords for vk in critical_terms]
        
        # Strong keywords
        strong = ValidationKeyword.query.filter_by(
            document_type_id=self.doc_type.id,
            keyword_type='strong_keywords'
        ).first()
        self.validation_keywords['strong_keywords'] = strong.keywords if strong else []
        
        # Excluded keywords
        excluded = ValidationKeyword.query.filter_by(
            document_type_id=self.doc_type.id,
            keyword_type='excluded_keywords'
        ).first()
        self.validation_keywords['excluded_keywords'] = excluded.keywords if excluded else []
        
        # OCR kullanƒ±mƒ± (service_file'a bakarak belirle - gelecekte config'e eklenebilir)
        # ≈ûimdilik default: Varsa strong_keywords varsa OCR kullan
        self.use_ocr = self.doc_type.needs_ocr
    
    def detect_language(self, text: str) -> str:
        """Metin dilini tespit et"""
        if not LANGUAGE_DETECTION_AVAILABLE:
            return 'tr'
        
        try:
            sample_text = text[:500].strip()
            if not sample_text:
                return 'tr'
            detected_lang = detect(sample_text)
            logger.info(f"Tespit edilen dil: {detected_lang}")
            return detected_lang
        except Exception as e:
            logger.warning(f"Dil tespiti ba≈üarƒ±sƒ±z: {e}")
            return 'tr'
    
    def extract_text_from_file(self, file_path: str) -> str:
        """Dosyadan metin √ßƒ±kar (OCR ayarƒ±na g√∂re)"""
        try:
            text = ""
            file_ext = os.path.splitext(file_path)[1].lower()
            
            if file_ext == '.pdf':
                # OCR KONTROL√ú - √ñNCE KARAR VER! üëá
                if self.doc_type.needs_ocr:
                    # OCR ƒ∞≈ûARETLƒ∞ ‚Üí Dƒ∞REKT OCR KULLAN!
                    logger.info("‚úÖ OCR etkin - Direkt OCR kullanƒ±lƒ±yor (PyPDF2 atlanƒ±yor)")
                    return self.extract_text_with_ocr(file_path)
                else:
                    # OCR ƒ∞≈ûARETSƒ∞Z ‚Üí SADECE PyPDF2
                    logger.info("üìÑ OCR devre dƒ±≈üƒ± - Sadece PyPDF2 kullanƒ±lƒ±yor")
                    with open(file_path, 'rb') as file:
                        reader = PyPDF2.PdfReader(file)
                        for page in reader.pages:
                            page_text = page.extract_text()
                            if page_text:
                                text += page_text + "\n"
                    
                    text = text.strip()
                    
                    if len(text) > 100:
                        logger.info(f"‚úÖ PyPDF2 ba≈üarƒ±lƒ± - {len(text)} karakter")
                        return text
                    else:
                        logger.error("‚ùå PyPDF2 yetersiz metin √ßƒ±kardƒ± - OCR devre dƒ±≈üƒ±!")
                        return ""
            
            elif file_ext in ['.jpg', '.jpeg', '.png']:
                # G√∂rsel dosyalar i√ßin OCR ZORUNLU!
                if self.doc_type.needs_ocr:
                    logger.info("üì∑ G√∂rsel dosya - OCR kullanƒ±lƒ±yor...")
                    return self.extract_text_with_ocr(file_path)
                else:
                    logger.error("‚ùå OCR devre dƒ±≈üƒ± - G√∂rsel dosya i≈ülenemez!")
                    return ""
            
            elif file_ext in ['.docx', '.doc']:
                return self.extract_text_from_docx(file_path)
            
            elif file_ext == '.txt':
                return self.extract_text_from_txt(file_path)
            
            else:
                logger.error(f"‚ùå Desteklenmeyen dosya formatƒ±: {file_ext}")
                return ""
            
        except Exception as e:
            logger.error(f"‚ùå Metin √ßƒ±karma hatasƒ±: {e}")
            return ""
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDF'den PyPDF2 ile metin √ßƒ±kar"""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                all_text = ""
                
                for page in pdf_reader.pages:
                    page_text = page.extract_text()
                    page_text = re.sub(r'\s+', ' ', page_text)
                    all_text += page_text + "\n"
                
                all_text = all_text.strip()
                logger.info(f"PDF'den {len(all_text)} karakter √ßƒ±karƒ±ldƒ±")
                return all_text
        except Exception as e:
            logger.error(f"PDF metin √ßƒ±karma hatasƒ±: {e}")
            return ""
    
    def extract_text_with_ocr(self, file_path: str) -> str:
        """OCR ile metin √ßƒ±kar"""
        try:
            pages = pdf2image.convert_from_path(file_path, dpi=200)
            all_text = ""
            
            for i, page in enumerate(pages):
                opencv_image = cv2.cvtColor(np.array(page), cv2.COLOR_RGB2BGR)
                gray = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2GRAY)
                processed = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
                
                text = pytesseract.image_to_string(processed, config='--oem 3 --psm 6 -l tur+eng')
                all_text += text + "\n"
                logger.info(f"OCR sayfa {i+1}/{len(pages)} tamamlandƒ±")
            
            logger.info(f"OCR ile {len(all_text)} karakter √ßƒ±karƒ±ldƒ±")
            return all_text.strip()
        except Exception as e:
            logger.error(f"OCR hatasƒ±: {e}")
            return ""
    
    def extract_text_from_docx(self, docx_path: str) -> str:
        """DOCX'den metin √ßƒ±kar"""
        try:
            doc = Document(docx_path)
            text = ""
            
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        text += cell.text + " "
                text += "\n"
            
            return text.strip()
        except Exception as e:
            logger.error(f"DOCX metin √ßƒ±karma hatasƒ±: {e}")
            return ""
    
    def extract_text_from_txt(self, txt_path: str) -> str:
        """TXT dosyasƒ±ndan metin √ßƒ±kar"""
        try:
            with open(txt_path, 'r', encoding='utf-8') as file:
                return file.read().strip()
        except:
            try:
                with open(txt_path, 'r', encoding='cp1254') as file:
                    return file.read().strip()
            except Exception as e:
                logger.error(f"TXT metin √ßƒ±karma hatasƒ±: {e}")
                return ""
    
    def analyze_criteria(self, text: str, category: str) -> Dict[str, DynamicAnalysisResult]:
        """Kriterleri analiz et"""
        results = {}
        criteria = self.criteria_details.get(category, {})
        
        for criterion_name, criterion_data in criteria.items():
            pattern = criterion_data["pattern"]
            weight = criterion_data["weight"]
            
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            
            # üëá YENƒ∞ - DEBUG LOG
            logger.info(f"   üîç {criterion_name}:")
            logger.info(f"      Pattern: {pattern[:150]}...")
            logger.info(f"      Sonu√ß: {len(matches)} match bulundu")
            if matches:
                logger.info(f"      √ñrnek: {str(matches[:2])}")
                
            if matches:
                content = f"Bulunan: {str(matches[:3])}"
                found = True
                
                if weight <= 2:
                    score = min(weight, len(matches))
                else:
                    score = min(weight, len(matches) * (weight // 2))
                    score = max(score, weight // 2)
            else:
                content = "Bulunamadƒ±"
                found = False
                score = 0
            
            results[criterion_name] = DynamicAnalysisResult(
                criteria_name=criterion_name,
                found=found,
                content=content,
                score=score,
                max_score=weight,
                details={"pattern_used": pattern, "matches_count": len(matches) if matches else 0}
            )
        
        return results
    
    def calculate_scores(self, analysis_results: Dict[str, Dict[str, DynamicAnalysisResult]]) -> Dict[str, Any]:
        """Puanlarƒ± hesapla"""
        category_scores = {}
        total_score = 0
        
        for category, results in analysis_results.items():
            category_max = self.criteria_weights[category]
            category_earned = sum(result.score for result in results.values())
            category_possible = sum(result.max_score for result in results.values())
            
            if category_possible > 0:
                percentage = (category_earned / category_possible) * 100
                normalized_score = (percentage / 100) * category_max
            else:
                percentage = 0
                normalized_score = 0
            
            category_scores[category] = {
                "earned": category_earned,
                "possible": category_possible,
                "normalized": round(normalized_score, 2),
                "max_weight": category_max,
                "percentage": round(percentage, 2)
            }
            
            total_score += normalized_score
        
        return {
            "category_scores": category_scores,
            "total_score": round(total_score, 2),
            "percentage": round(total_score, 2)
        }
    
    def extract_specific_values(self, text: str) -> Dict[str, Any]:
        """D√∂k√ºman'a √∂zg√º deƒüerleri √ßƒ±kar"""
        values = {}
        
        extract_values = self.pattern_definitions.get('extract_values', {})
        
        for field_name, patterns_list in extract_values.items():
            values[field_name] = "Bulunamadƒ±"
            
            for pattern in patterns_list:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    try:
                        values[field_name] = match.group(1).strip()
                    except:
                        values[field_name] = match.group(0).strip()
                    break
        
        return values
    
    def generate_recommendations(self, analysis_results: Dict, scores: Dict) -> List[str]:
        """Detaylƒ± √∂neriler olu≈ütur - Eksik kriterleri g√∂ster"""
        recommendations = []
        
        total_percentage = scores["percentage"]
        
        # Genel durum
        if total_percentage >= 70:
            recommendations.append(f"‚úÖ {self.doc_type.name} GE√áERLƒ∞ (Toplam: %{total_percentage:.1f})")
        else:
            recommendations.append(f"‚ùå {self.doc_type.name} GE√áERSƒ∞Z (Toplam: %{total_percentage:.1f})")
        
        # Kategori bazlƒ± detaylƒ± analiz
        for category, results in analysis_results.items():
            category_score = scores["category_scores"][category]["percentage"]
            category_earned = scores["category_scores"][category]["earned"]
            category_possible = scores["category_scores"][category]["possible"]
            
            # Kategori durumu
            if category_score < 40:
                status = "YETERSƒ∞Z"
            elif category_score < 70:
                status = "GELƒ∞≈ûTƒ∞Rƒ∞LMELƒ∞"
            else:
                status = "YETERLƒ∞"
            
            # Eksik/Zayƒ±f kriterleri topla
            criteria_details = []
            
            for criterion_name, result in results.items():
                if result.score == 0:
                    # Hi√ß puan alamamƒ±≈ü
                    criteria_details.append(f"{criterion_name}: Bulunamadƒ± (0/{result.max_score} puan)")
                elif result.score < result.max_score * 0.5:
                    # %50'den az puan almƒ±≈ü
                    criteria_details.append(f"{criterion_name}: Yetersiz ({result.score}/{result.max_score} puan)")
                elif result.score < result.max_score:
                    # Tam puan almamƒ±≈ü ama %50'den fazla
                    criteria_details.append(f"{criterion_name}: Geli≈ütirilmeli ({result.score}/{result.max_score} puan)")
            
            # Kategori satƒ±rƒ± - Emoji + Kategori + Puan + Kriterler
            if criteria_details:
                # Eksik kriterler varsa listele
                emoji = "üî¥" if category_score < 40 else "üü°" if category_score < 70 else "üü¢"
                criteria_str = ", ".join(criteria_details)
                recommendations.append(
                    f"{emoji} {category}: {criteria_str}"
                )
            else:
                # T√ºm kriterler tam puan
                recommendations.append(
                    f"üü¢ {category}: T√ºm kriterler yeterli (%{category_score:.1f})"
                )
        
        return recommendations
    
    def analyze_document(self, file_path: str) -> Dict[str, Any]:
        """Ana analiz fonksiyonu"""
        logger.info(f"{self.doc_type.name} analizi ba≈ülatƒ±lƒ±yor...")
        
        if not os.path.exists(file_path):
            return {"error": f"Dosya bulunamadƒ±: {file_path}"}
        
        text = self.extract_text_from_file(file_path)
        if not text:
            return {"error": "Dosyadan metin √ßƒ±karƒ±lamadƒ±"}
        
        detected_lang = self.detect_language(text)
        
        analysis_results = {}
        for category in self.criteria_weights.keys():
            analysis_results[category] = self.analyze_criteria(text, category)
        
        scores = self.calculate_scores(analysis_results)
        extracted_values = self.extract_specific_values(text)
        recommendations = self.generate_recommendations(analysis_results, scores)
        
        final_status = "PASS" if scores["percentage"] >= 70 else "FAIL"
        
        return {
            "analiz_tarihi": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "dosya_bilgisi": {
                "file_path": file_path,
                "file_type": os.path.splitext(file_path)[1].lower(),
                "detected_language": detected_lang
            },
            "cikarilan_degerler": extracted_values,
            "kategori_analizleri": analysis_results,
            "puanlama": scores,
            "oneriler": recommendations,
            "ozet": {
                "toplam_puan": scores["total_score"],
                "yuzde": scores["percentage"],
                "durum": final_status,
                "rapor_tipi": self.doc_type.code.upper()
            }
        }


# ============================================
# HELPER FUNCTIONS - VALIDATION
# ============================================
def validate_document_server(text, validation_keywords):
    """D√∂k√ºman validasyonu"""
    critical_terms_data = validation_keywords.get('critical_terms', [])
    
    if not critical_terms_data:
        logger.warning("‚ö†Ô∏è Critical terms bulunamadƒ±, validasyon atlanƒ±yor")
        return True
    
    category_found = []
    
    for i, category in enumerate(critical_terms_data):
        found_in_category = False
        for term in category:
            if re.search(rf"\b{term}\b", text, re.IGNORECASE):
                found_in_category = True
                logger.info(f"Kategori {i+1} bulundu: '{term}'")
                break
        category_found.append(found_in_category)
    
    valid_categories = sum(category_found)
    logger.info(f"D√∂k√ºman validasyonu: {valid_categories}/{len(critical_terms_data)} kritik kategori bulundu")
    
    return valid_categories >= len(critical_terms_data) - 1


def check_strong_keywords_first_pages(filepath, validation_keywords):
    """ƒ∞lk 1-2 sayfada √∂zg√º kelimeleri OCR ile ara"""
    strong_keywords = validation_keywords.get('strong_keywords', [])
    
    if not strong_keywords:
        logger.warning("‚ö†Ô∏è Strong keywords bulunamadƒ±, validasyon atlanƒ±yor")
        return True
    
    try:
        pages = pdf2image.convert_from_path(filepath, dpi=200, first_page=1, last_page=2)
        
        all_text = ""
        for page in pages:
            opencv_image = cv2.cvtColor(np.array(page), cv2.COLOR_RGB2BGR)
            gray = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2GRAY)
            processed = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
            
            text = pytesseract.image_to_string(processed, config='--oem 3 --psm 6 -l tur+eng')
            all_text += text.lower() + " "
        
        found_keywords = [kw for kw in strong_keywords if re.search(rf"\b{kw.lower()}\b", all_text)]
        logger.info(f"ƒ∞lk sayfa kontrol: {len(found_keywords)} √∂zg√º kelime")
        return len(found_keywords) >= 1
    except Exception as e:
        logger.warning(f"ƒ∞lk sayfa kontrol hatasƒ±: {e}")
        return False


def check_excluded_keywords_first_pages(filepath, validation_keywords):
    """ƒ∞lk 1-2 sayfada istenmeyen kelimeleri ara"""
    excluded_keywords = validation_keywords.get('excluded_keywords', [])
    
    if not excluded_keywords:
        logger.warning("‚ö†Ô∏è Excluded keywords bulunamadƒ±, validasyon atlanƒ±yor")
        return False
    
    try:
        pages = pdf2image.convert_from_path(filepath, dpi=200, first_page=1, last_page=2)
        
        all_text = ""
        for page in pages:
            opencv_image = cv2.cvtColor(np.array(page), cv2.COLOR_RGB2BGR)
            gray = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2GRAY)
            processed = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
            
            text = pytesseract.image_to_string(processed, config='--oem 3 --psm 6 -l tur+eng')
            all_text += text.lower() + " "
        
        found_excluded = [kw for kw in excluded_keywords if re.search(rf"\b{kw.lower()}\b", all_text)]
        logger.info(f"ƒ∞lk sayfa excluded kontrol: {len(found_excluded)} istenmeyen kelime - Bulunanlar: {found_excluded}") 
        return len(found_excluded) >= 1
    except Exception as e:
        logger.warning(f"ƒ∞lk sayfa excluded kontrol hatasƒ±: {e}")
        return False


# ============================================
# FLASK SERVICE
# ============================================
app = Flask(__name__)

# Database configuration
app.config['SQLALCHEMY_DATABASE_URI'] = Config.SQLALCHEMY_DATABASE_URI
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = Config.SQLALCHEMY_TRACK_MODIFICATIONS
app.config['SQLALCHEMY_ECHO'] = Config.SQLALCHEMY_ECHO

UPLOAD_FOLDER = 'temp_uploads_dynamic'
ALLOWED_EXTENSIONS = {'pdf', 'docx', 'doc', 'txt'}

if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024


def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


@app.route('/api/dynamic-report', methods=['POST'])
def analyze_dynamic_report():
    """Dynamic d√∂k√ºman analiz endpoint - 3 A≈üamalƒ± Validasyon"""
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'No file provided'}), 400

        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400

        if not allowed_file(file.filename):
            return jsonify({'error': 'Invalid file type'}), 400

        # Document code al
        document_code = request.form.get('document_code')
        if not document_code:
            return jsonify({'error': 'document_code gerekli'}), 400

        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)
        
        logger.info(f"Dynamic analiz ba≈ülatƒ±lƒ±yor: {document_code} - {filename}")

        # Analyzer olu≈ütur
        analyzer = DynamicReportAnalyzer(document_code, app=app)
        
        # √ú√á A≈ûAMALI KONTROL (sadece PDF i√ßin)
        file_ext = os.path.splitext(filepath)[1].lower()
        
        if file_ext == '.pdf':
            logger.info("A≈üama 1: ƒ∞lk sayfa √∂zg√º kelime kontrol√º...")
            if check_strong_keywords_first_pages(filepath, analyzer.validation_keywords):
                logger.info("‚úÖ A≈üama 1 ge√ßti")
            else:
                logger.info("A≈üama 2: ƒ∞lk sayfa excluded kelime kontrol√º...")
                if check_excluded_keywords_first_pages(filepath, analyzer.validation_keywords):
                    logger.info("‚ùå A≈üama 2'de excluded kelimeler bulundu")
                    try:
                        os.remove(filepath)
                    except:
                        pass
                    return jsonify({
                        'error': 'Invalid document type',
                        'message': f'Bu dosya {analyzer.doc_type.name} deƒüil (farklƒ± rapor t√ºr√º tespit edildi).'
                    }), 400
                else:
                    logger.info("A≈üama 3: Tam dok√ºman critical terms kontrol√º...")
                    try:
                        with open(filepath, 'rb') as pdf_file:
                            pdf_reader = PyPDF2.PdfReader(pdf_file)
                            text = ""
                            for page in pdf_reader.pages:
                                text += page.extract_text() + "\n"
                        
                        if not text or len(text.strip()) == 0:
                            try:
                                os.remove(filepath)
                            except:
                                pass
                            return jsonify({'error': 'Text extraction failed'}), 400
                        
                        if not validate_document_server(text, analyzer.validation_keywords):
                            try:
                                os.remove(filepath)
                            except:
                                pass
                            return jsonify({
                                'error': 'Invalid document type',
                                'message': f'Y√ºklediƒüiniz dosya {analyzer.doc_type.name} deƒüil!'
                            }), 400
                    except Exception as e:
                        logger.error(f"A≈üama 3 hatasƒ±: {e}")
                        try:
                            os.remove(filepath)
                        except:
                            pass
                        return jsonify({'error': 'Analysis failed'}), 500

        # Analizi yap
        logger.info(f"D√∂k√ºman doƒürulandƒ±, analiz ba≈ülatƒ±lƒ±yor: {filename}")
        analysis_result = analyzer.analyze_document(filepath)
        
        try:
            os.remove(filepath)
        except:
            pass

        if 'error' in analysis_result:
            return jsonify({'error': 'Analysis failed', 'message': analysis_result['error']}), 400

        overall_percentage = analysis_result['puanlama']['percentage']
        status = "PASS" if overall_percentage >= 70 else "FAIL"
        
        response_data = {
            'analysis_date': analysis_result.get('analiz_tarihi'),
            'analysis_id': f"{document_code}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'category_scores': {},
            'extracted_values': analysis_result.get('cikarilan_degerler', {}),
            'file_type': document_code.upper(),
            'filename': filename,
            'overall_score': {
                'percentage': round(overall_percentage, 2),
                'total_points': analysis_result['puanlama']['total_score'],
                'max_points': 100,
                'status': status,
                'status_tr': 'GE√áERLƒ∞' if status == "PASS" else 'GE√áERSƒ∞Z'
            },
            'recommendations': analysis_result.get('oneriler', []),
            'summary': {
                'is_valid': status == "PASS",
                'conclusion': f"{analyzer.doc_type.name} analiz sonucu: {status}",
                'main_issues': []
            }
        }
        
        for category, score_data in analysis_result['puanlama']['category_scores'].items():
            response_data['category_scores'][category] = {
                'score': score_data['normalized'],
                'max_score': score_data['max_weight'],
                'percentage': score_data['percentage'],
                'status': 'PASS' if score_data['percentage'] >= 70 else 'FAIL'
            }

        return jsonify({
            'success': True,
            'message': f'{analyzer.doc_type.name} ba≈üarƒ±yla analiz edildi',
            'analysis_service': 'dynamic',
            'data': response_data
        })

    except Exception as e:
        logger.error(f"API error: {str(e)}")
        return jsonify({'error': 'Server error', 'message': str(e)}), 500


@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check"""
    return jsonify({
        'status': 'healthy',
        'service': 'Dynamic Document Analyzer API',
        'version': '1.0.0'
    })


@app.route('/', methods=['GET'])
def index():
    """Index page"""
    return jsonify({
        'service': 'Dynamic Document Analyzer API',
        'version': '1.0.0',
        'endpoints': {
            'POST /api/dynamic-report': 'Dynamic d√∂k√ºman analizi',
            'GET /api/health': 'Health check',
            'GET /': 'API info'
        }
    })


# ============================================
# DATABASE INITIALIZATION
# ============================================
with app.app_context():
    db.init_app(app)


# ============================================
# APPLICATION ENTRY POINT (Azure-Friendly)
# ============================================
if __name__ == '__main__':
    logger.info("=" * 60)
    logger.info("Dynamic Document Analyzer Service")
    logger.info("=" * 60)
    
    port = int(os.environ.get('PORT', 8018))
    logger.info(f"üöÄ Servis ba≈ülatƒ±lƒ±yor - Port: {port}")
    
    app.run(host='0.0.0.0', port=port, debug=False)