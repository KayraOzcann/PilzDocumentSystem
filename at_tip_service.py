"""
AT Tip ƒ∞nceleme Sertifikasƒ± Analiz Servisi - Database Entegrasyonlu
==========================================
Endpoint: POST /api/at-type-cert-report
Health: GET /api/health
"""

import PyPDF2
import cv2
import numpy as np
from PIL import Image
import pdf2image
import pytesseract
from flask import Flask, request, jsonify
import os
import re
from datetime import datetime
from werkzeug.utils import secure_filename
import logging
from dataclasses import dataclass
from typing import Dict, Any, List
from docx import Document

# ============================================
# DATABASE IMPORTS (YENƒ∞)
# ============================================
from flask import current_app
from database import db, init_db
from db_loader import load_service_config
from config import Config

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

try:
    from langdetect import detect
    LANG_DETECT = True
except:
    LANG_DETECT = False

@dataclass
class ATTipIncelemeResult:
    criteria_name: str
    found: bool
    content: str
    score: int
    max_score: int
    is_critical: bool
    details: Dict[str, Any]

class ATTipIncelemeAnalyzer:
    def __init__(self, app=None):
        logger.info("AT Type-Examination Certificate analysis system starting...")
        
        # Flask app context varsa DB'den y√ºkle, yoksa bo≈ü ba≈ülat
        if app:
            with app.app_context():
                try:
                    config = load_service_config('at_type_report')
                    
                    # DB'den y√ºklenen veriler
                    self.criteria_weights = config.get('criteria_weights', {})
                    self.criteria_details = config.get('criteria_details', {})
                    self.pattern_definitions = config.get('pattern_definitions', {})
                    self.validation_keywords = config.get('validation_keywords', {})
                    self.category_actions = config.get('category_actions', {})
                    
                    logger.info(f"‚úÖ Veritabanƒ±ndan y√ºklendi: {len(self.criteria_weights)} kategori")
                    
                except Exception as e:
                    logger.error(f"‚ö†Ô∏è Veritabanƒ±ndan y√ºkleme ba≈üarƒ±sƒ±z: {e}")
                    logger.warning("‚ö†Ô∏è Fallback: Bo≈ü config kullanƒ±lƒ±yor")
                    self.criteria_weights = {}
                    self.criteria_details = {}
                    self.pattern_definitions = {}
                    self.validation_keywords = {}
                    self.category_actions = {}
        else:
            # Flask app yoksa bo≈ü ba≈ülat (eski davranƒ±≈ü)
            logger.warning("‚ö†Ô∏è Flask app context yok, bo≈ü config kullanƒ±lƒ±yor")
            self.criteria_weights = {}
            self.criteria_details = {}
            self.pattern_definitions = {}
            self.validation_keywords = {}
            self.category_actions = {}

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDF'den metin √ßƒ±karƒ±mƒ± - PyPDF2 + OCR fallback"""
        text = ""
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                for page in reader.pages:
                    text += page.extract_text() or ""
            
            logger.info(f"PyPDF2 extracted {len(text)} characters")
            
            if len(text.strip()) < 50:
                logger.info("Insufficient text with PyPDF2, trying OCR...")
                pages = pdf2image.convert_from_path(pdf_path, dpi=200)
                ocr_text = ""
                
                for i, page in enumerate(pages, 1):
                    try:
                        page_text = pytesseract.image_to_string(page, lang='tur+eng+deu+fra+spa')
                        ocr_text += page_text + "\n"
                        logger.info(f"OCR extracted {len(page_text)} characters from page {i}")
                    except Exception as e:
                        logger.warning(f"OCR failed for page {i}: {e}")
                        continue
                
                if len(ocr_text.strip()) > len(text.strip()):
                    text = ocr_text
                    logger.info(f"OCR total text length: {len(text)}")
        
        except Exception as e:
            logger.error(f"Error extracting text: {e}")
            return ""
        
        return text

    def extract_text_from_docx(self, docx_path: str) -> str:
        """DOCX'den metin √ßƒ±karƒ±mƒ±"""
        try:
            doc = Document(docx_path)
            return "\n".join([p.text for p in doc.paragraphs])
        except Exception as e:
            logger.error(f"DOCX hatasƒ±: {e}")
            return ""

    def extract_text_from_txt(self, txt_path: str) -> str:
        """TXT'den metin √ßƒ±karƒ±mƒ±"""
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception:
            return ""

    def detect_language(self, text: str) -> str:
        if not LANG_DETECT:
            return 'en'
        try:
            return detect(text[:500].strip()) if len(text.strip()) >= 50 else "en"
        except:
            return 'en'
        
    def analyze_criteria(self, text: str, category: str) -> Dict[str, ATTipIncelemeResult]:
        """Kriterleri analiz et - DB'den gelen pattern'lerle"""
        results = {}
        criteria = self.criteria_details.get(category, {})
        
        for criterion_name, criterion_data in criteria.items():
            pattern = criterion_data.get("pattern", "")
            weight = criterion_data.get("weight", 0)
            # critical veya is_critical olabilir - her ikisini de kontrol et
            is_critical = criterion_data.get("critical", criterion_data.get("is_critical", False))
            description = criterion_data.get("description", criterion_name)  # Fallback: criterion_name
            
            if not pattern:  # Pattern yoksa atla
                logger.warning(f"Pattern bulunamadƒ±: {criterion_name}")
                continue
            
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            
            if matches:
                # Clean up matches and get the best one
                clean_matches = []
                for match in matches:
                    if isinstance(match, tuple):
                        # For groups in regex, take the first non-empty group
                        clean_match = next((m for m in match if m.strip()), "")
                    else:
                        clean_match = str(match)
                    
                    if clean_match.strip():
                        clean_matches.append(clean_match.strip())
                
                if clean_matches:
                    content = f"Bulundu: {clean_matches[0][:60]}..."
                    found = True
                    score = weight  # Full points
                else:
                    content = "E≈üle≈üme bulundu ama deƒüer √ßƒ±karƒ±lamadƒ±"
                    found = True
                    score = int(weight * 0.5)  # Partial points
            else:
                content = "Bulunamadƒ±"
                found = False
                score = 0
            
            results[criterion_name] = ATTipIncelemeResult(
                criteria_name=criterion_name,
                found=found,
                content=content,
                score=score,
                max_score=weight,
                is_critical=is_critical,
                details={
                    "description": description,
                    "pattern_used": pattern,
                    "matches_count": len(matches) if matches else 0,
                    "raw_matches": matches[:3] if matches else []
                }
            )
        
        return results

    def calculate_scores(self, analysis_results: Dict[str, Dict[str, ATTipIncelemeResult]]) -> Dict[str, Any]:
        """Puanlama hesaplama"""
        category_scores = {}
        total_score = 0
        critical_missing = []
        
        for category, results in analysis_results.items():
            category_max = self.criteria_weights[category]
            category_earned = sum(result.score for result in results.values())
            category_possible = sum(result.max_score for result in results.values())
            
            for criterion_name, result in results.items():
                if result.is_critical and not result.found:
                    critical_missing.append(f"{category}: {result.details['description']}")
            
            if category_possible > 0:
                percentage = (category_earned / category_possible) * 100
                normalized_score = (percentage / 100) * category_max
            else:
                percentage = 0
                normalized_score = 0
            
            category_scores[category] = {
                "earned": category_earned,
                "possible": category_possible,
                "normalized": round(normalized_score, 2),
                "max_weight": category_max,
                "percentage": round(percentage, 2)
            }
            
            total_score += normalized_score
        
        return {
            "category_scores": category_scores,
            "total_score": round(total_score, 2),
            "percentage": round(total_score, 2),
            "critical_missing": critical_missing
        }

    def extract_specific_values(self, text: str) -> Dict[str, Any]:
        """Spesifik deƒüerleri √ßƒ±kar - DB'den pattern'lerle - ORƒ∞Jƒ∞NAL MANTIK"""
        values = {
            "notified_body_name": "Bulunamadƒ±",
            "notified_body_address": "Bulunamadƒ±",
            "notified_body_id": "Bulunamadƒ±",
            "manufacturer_name": "Bulunamadƒ±",
            "manufacturer_address": "Bulunamadƒ±",
            "machine_trade_name": "Bulunamadƒ±",
            "machine_type": "Bulunamadƒ±",
            "machine_model": "Bulunamadƒ±",
            "serial_number": "Bulunamadƒ±",
            "certificate_number": "Bulunamadƒ±",
            "issue_date": "Bulunamadƒ±",
            "validity_date": "Bulunamadƒ±",
            "directive_reference": "Bulunamadƒ±",
            "applied_standards": [],
            "authorized_person": "Bulunamadƒ±"
        }

        # DB'den extract_values pattern'lerini al
        extract_patterns = self.pattern_definitions.get('extract_values', {})

        # Notified Body Name - DB'den pattern'ler
        nb_name_patterns = extract_patterns.get('notified_body_name', [])
        for pattern in nb_name_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                values["notified_body_name"] = match.group(1).strip()
                break

        # Notified Body ID - DB'den pattern'ler
        nb_id_patterns = extract_patterns.get('notified_body_id', [])
        for pattern in nb_id_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                values["notified_body_id"] = match.group(1).strip()
                break

        # Manufacturer Name - DB'den pattern'ler
        manuf_patterns = extract_patterns.get('manufacturer_name', [])
        for pattern in manuf_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                values["manufacturer_name"] = match.group(1).strip()
                break

        # Machine Type/Model - DB'den pattern'ler
        machine_patterns = extract_patterns.get('machine_type', [])
        for pattern in machine_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                values["machine_type"] = match.group(1).strip()
                break

        # Certificate Number - DB'den pattern'ler
        cert_patterns = extract_patterns.get('certificate_number', [])
        for pattern in cert_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                cert_num = match.group(1).strip()
                if len(cert_num) >= 5:
                    values["certificate_number"] = cert_num
                    break

        # Issue Date - DB'den pattern'ler
        date_patterns = extract_patterns.get('issue_date', [])
        for pattern in date_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                values["issue_date"] = match.group(1).strip()
                break

        # Serial Number - DB'den pattern'ler
        serial_patterns = extract_patterns.get('serial_number', [])
        for pattern in serial_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                values["serial_number"] = match.group(1).strip()
                break

        # Applied Standards - DB'den pattern (liste d√∂nd√ºr√ºr)
        standards_patterns = extract_patterns.get('applied_standards', [])
        if standards_patterns:
            standards = re.findall(standards_patterns[0], text, re.IGNORECASE)
            values["applied_standards"] = list(set(standards))

        # Directive Reference - DB'den pattern (kontrol)
        directive_patterns = extract_patterns.get('directive_reference', [])
        if directive_patterns:
            if re.search(directive_patterns[0], text, re.IGNORECASE):
                values["directive_reference"] = "2006/42/EC"

        return values

    def generate_recommendations(self, analysis_results: Dict[str, Dict[str, ATTipIncelemeResult]], 
                                scores: Dict[str, Any]) -> List[str]:
        """√ñneriler olu≈ütur"""
        recommendations = []
        
        if scores["critical_missing"]:
            recommendations.append("üö® KRƒ∞Tƒ∞K EKSƒ∞KLƒ∞KLER - BELGE GE√áERSƒ∞ZDƒ∞R!")
            recommendations.append("‚ö†Ô∏è 2006/42/EC Ek IX'a g√∂re a≈üaƒüƒ±daki bilgilerden biri eksikse belge ge√ßersizdir:")
            for missing in scores["critical_missing"]:
                recommendations.append(f"  ‚ùå {missing}")
            recommendations.append("")
        
        for category, score_data in scores["category_scores"].items():
            if score_data["percentage"] < 100:
                missing_items = []
                for criterion_name, result in analysis_results[category].items():
                    if result.is_critical and not result.found:
                        missing_items.append(result.details['description'])
                if missing_items:
                    recommendations.append(f"üö® {category} - Kritik Eksikler:")
                    for item in missing_items:
                        recommendations.append(f"  ‚ùå {item}")
        
        total_percentage = scores["percentage"]
        critical_missing_count = len(scores["critical_missing"])
        
        if critical_missing_count > 0:
            recommendations.append("üî¥ SONU√á: BELGE GE√áERSƒ∞ZDƒ∞R")
            recommendations.append("‚öñÔ∏è Hukuki Durum: 2006/42/EC Ek IX gereksinimlerini kar≈üƒ±lamƒ±yor")
            recommendations.append("üîß Acil Eylem: Eksik bilgileri tamamlayarak yeni belge d√ºzenlenmeli")
        elif total_percentage >= 90:
            recommendations.append("‚úÖ SONU√á: BELGE TAM UYGUNLUKTA")
            recommendations.append("‚öñÔ∏è Hukuki Durum: 2006/42/EC Ek IX gereksinimlerini tam kar≈üƒ±lƒ±yor")
            recommendations.append("üìã Durum: AT Tip ƒ∞ncelemesi Belgesi hukuken ge√ßerlidir")
        elif total_percentage >= 80:
            recommendations.append("üü° SONU√á: BELGE KABUL EDƒ∞LEBƒ∞Lƒ∞R")
            recommendations.append("‚öñÔ∏è Hukuki Durum: Temel gereksinimleri kar≈üƒ±lƒ±yor")
            recommendations.append("üí° √ñneri: Teknik detaylar geli≈ütirilebilir")
        else:
            recommendations.append("üü† SONU√á: BELGE YETERSƒ∞Z")
            recommendations.append("‚öñÔ∏è Hukuki Durum: √ñnemli eksiklikler mevcut")
            recommendations.append("üîç √ñneri: Belge g√∂zden ge√ßirilmeli")
        
        return recommendations

    def analyze_type_examination_certificate(self, file_path: str) -> Dict[str, Any]:
        """Ana analiz fonksiyonu"""
        logger.info("Type-Examination Certificate analysis starting...")
        
        try:
            if not os.path.exists(file_path):
                return {"error": f"Dosya bulunamadƒ±: {file_path}"}
            
            file_ext = os.path.splitext(file_path)[1].lower()
            
            # Metin √ßƒ±karƒ±mƒ±
            if file_ext == '.pdf':
                text = self.extract_text_from_pdf(file_path)
            elif file_ext in ['.docx', '.doc']:
                text = self.extract_text_from_docx(file_path)
            elif file_ext == '.txt':
                text = self.extract_text_from_txt(file_path)
            else:
                return {"error": f"Desteklenmeyen dosya formatƒ±: {file_ext}"}
            
            if len(text.strip()) < 50:
                return {
                    "error": "Dosyadan yeterli metin √ßƒ±karƒ±lamadƒ±",
                    "text_length": len(text)
                }
            
            detected_language = self.detect_language(text)
            logger.info(f"Detected language: {detected_language}")
            
            extracted_values = self.extract_specific_values(text)
            
            category_analyses = {}
            for category in self.criteria_weights.keys():
                category_analyses[category] = self.analyze_criteria(text, category)
            
            scoring = self.calculate_scores(category_analyses)
            recommendations = self.generate_recommendations(category_analyses, scoring)
            
            percentage = scoring["percentage"]
            has_critical_missing = len(scoring["critical_missing"]) > 0
            
            if has_critical_missing:
                status = "INVALID"
                status_tr = "GE√áERSƒ∞Z"
            elif percentage >= 90:
                status = "FULLY_COMPLIANT"
                status_tr = "TAM UYGUNLUK"
            elif percentage >= 80:
                status = "ACCEPTABLE"
                status_tr = "KABUL EDƒ∞LEBƒ∞Lƒ∞R"
            elif percentage >= 70:
                status = "CONDITIONAL"
                status_tr = "KO≈ûULLU"
            else:
                status = "INSUFFICIENT"
                status_tr = "YETERSƒ∞Z"
            
            return {
                "analysis_date": datetime.now().isoformat(),
                "file_info": {
                    "filename": os.path.basename(file_path),
                    "text_length": len(text),
                    "detected_language": detected_language
                },
                "extracted_values": extracted_values,
                "category_analyses": category_analyses,
                "scoring": scoring,
                "recommendations": recommendations,
                "summary": {
                    "total_score": scoring["total_score"],
                    "percentage": percentage,
                    "status": status,
                    "status_tr": status_tr,
                    "critical_missing_count": len(scoring["critical_missing"]),
                    "report_type": "AT_TIP_INCELEME_SERTIFIKASI"
                }
            }
        
        except Exception as e:
            logger.error(f"Analysis error: {e}")
            return {
                "error": f"Analiz sƒ±rasƒ±nda hata olu≈ütu: {str(e)}",
                "analysis_date": datetime.now().isoformat()
            }

# ============================================
# HELPER FUNCTIONS (Server Validasyon)
# ============================================
def validate_document_server(text, validation_keywords):
    """Server kodunda dok√ºman validasyonu - DB'den gelen keywords ile"""
    
    # DB'den gelen critical_terms
    critical_terms_data = validation_keywords.get('critical_terms', [])
    
    # Liste formatƒ±na d√∂n√º≈üt√ºr
    critical_terms = []
    for item in critical_terms_data:
        if isinstance(item, dict) and 'keywords' in item:
            critical_terms.append(item['keywords'])
        elif isinstance(item, list):
            critical_terms.append(item)
    
    if not critical_terms:
        logger.warning("‚ö†Ô∏è Critical terms bulunamadƒ±, validasyon atlanƒ±yor")
        return True
    
    category_found = []
    for i, category in enumerate(critical_terms):
        found_in_category = False
        for term in category:
            if re.search(rf"\b{term}\b", text, re.IGNORECASE):
                found_in_category = True
                logger.info(f"AT Type Cert Kategori {i+1} bulundu: '{term}'")
                break
        category_found.append(found_in_category)
    
    valid_categories = sum(category_found)
    logger.info(f"Dok√ºman validasyonu: {valid_categories}/{len(critical_terms)} kritik kategori bulundu")
    return valid_categories >= len(critical_terms) - 1


def check_strong_keywords_first_pages(filepath, validation_keywords):
    """ƒ∞lk 1-2 sayfada √∂zg√º kelimeleri OCR ile ara - DB'den keywords"""
    strong_keywords = validation_keywords.get('strong_keywords', [])
    
    if not strong_keywords:
        logger.warning("‚ö†Ô∏è Strong keywords bulunamadƒ±, validasyon atlanƒ±yor")
        return True
    
    try:
        pages = pdf2image.convert_from_path(filepath, dpi=300, first_page=1, last_page=1)
        
        all_text = ""
        for page in pages:
            opencv_image = cv2.cvtColor(np.array(page), cv2.COLOR_RGB2BGR)
            gray = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2GRAY)
            processed = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
            text = pytesseract.image_to_string(processed, config='--oem 3 --psm 6 -l eng+tur')
            all_text += text.lower() + " "
        
        found_keywords = []
        for keyword in strong_keywords:
            if re.search(rf"\b{keyword.lower()}\b", all_text):
                found_keywords.append(keyword)
        
        logger.info(f"ƒ∞lk sayfa kontrol: {len(found_keywords)} √∂zg√º kelime: {found_keywords}")
        return len(found_keywords) >= 1
        
    except Exception as e:
        logger.warning(f"ƒ∞lk sayfa kontrol hatasƒ±: {e}")
        return False


def check_excluded_keywords_first_pages(filepath, validation_keywords):
    """ƒ∞lk 1-2 sayfada istenmeyen rapor t√ºrlerinin kelimelerini ara - DB'den"""
    excluded_keywords = validation_keywords.get('excluded_keywords', [])
    
    if not excluded_keywords:
        logger.warning("‚ö†Ô∏è Excluded keywords bulunamadƒ±, validasyon atlanƒ±yor")
        return False
    
    try:
        pages = pdf2image.convert_from_path(filepath, dpi=200, first_page=1, last_page=2)
        
        all_text = ""
        for page in pages:
            opencv_image = cv2.cvtColor(np.array(page), cv2.COLOR_RGB2BGR)
            gray = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2GRAY)
            processed = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
            text = pytesseract.image_to_string(processed, config='--oem 3 --psm 6 -l eng+tur')
            all_text += text.lower() + " "
        
        found_excluded = []
        for keyword in excluded_keywords:
            if re.search(rf"\b{keyword.lower()}\b", all_text):
                found_excluded.append(keyword)
        
        logger.info(f"ƒ∞lk sayfa excluded kontrol: {len(found_excluded)} istenmeyen kelime: {found_excluded}")
        return len(found_excluded) >= 1
        
    except Exception as e:
        logger.warning(f"ƒ∞lk sayfa excluded kontrol hatasƒ±: {e}")
        return False


def get_conclusion_message(status, percentage):
    if status == "PASS":
        return f"AT Tip ƒ∞nceleme Sertifikasƒ± 2006/42/EC Ek IX'a uygun (%{percentage:.0f})"
    return f"AT Tip ƒ∞nceleme Sertifikasƒ± direktife uygun deƒüil (%{percentage:.0f})"


def get_main_issues(report):
    issues = []
    if report['scoring']['critical_missing']:
        for item in report['scoring']['critical_missing']:
            issues.append(f"Kritik eksik: {item}")
    return issues[:4]


# ============================================
# FLASK SERVƒ∞S KATMANI - CONFIGURATION
# ============================================
app = Flask(__name__)

# Database configuration (YENƒ∞)
app.config['SQLALCHEMY_DATABASE_URI'] = Config.SQLALCHEMY_DATABASE_URI
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = Config.SQLALCHEMY_TRACK_MODIFICATIONS
app.config['SQLALCHEMY_ECHO'] = Config.SQLALCHEMY_ECHO

UPLOAD_FOLDER = 'temp_uploads_at_type_cert'
ALLOWED_EXTENSIONS = {'pdf', 'docx', 'doc', 'txt'}

if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 32 * 1024 * 1024


def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


# ============================================
# FLASK SERVƒ∞S KATMANI - API ENDPOINTS
# ============================================
@app.route('/api/at-type-cert-report', methods=['POST'])
def analyze_at_type_cert_report():
    """AT Type Certificate analiz API endpoint'i - 3 A≈üamalƒ± Validasyon"""
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'No file provided'}), 400
        
        file = request.files['file']
        
        if file.filename == '' or not allowed_file(file.filename):
            return jsonify({'error': 'Invalid file'}), 400
        
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)
        
        # Create analyzer instance with app context
        analyzer = ATTipIncelemeAnalyzer(app=app)
        
        file_ext = os.path.splitext(filepath)[1].lower()
        
        # √ú√á A≈ûAMALI AT TYPE CERTIFICATE KONTROL√ú
        logger.info(f"√ú√ß a≈üamalƒ± AT Type Certificate kontrol√º ba≈ülatƒ±lƒ±yor: {filename}")
        
        if file_ext == '.pdf':
            logger.info("A≈üama 1: ƒ∞lk sayfa AT Type Certificate √∂zg√º kelime kontrol√º...")
            if check_strong_keywords_first_pages(filepath, analyzer.validation_keywords):
                logger.info("‚úÖ A≈üama 1 ge√ßti - AT Type Certificate √∂zg√º kelimeler bulundu")
            else:
                logger.info("A≈üama 2: ƒ∞lk sayfa excluded kelime kontrol√º...")
                if check_excluded_keywords_first_pages(filepath, analyzer.validation_keywords):
                    logger.info("‚ùå A≈üama 2'de excluded kelimeler bulundu - AT Type Certificate deƒüil")
                    try:
                        os.remove(filepath)
                    except:
                        pass
                    return jsonify({
                        'error': 'Invalid document type',
                        'message': 'Bu dosya AT Type Certificate deƒüil (farklƒ± rapor t√ºr√º tespit edildi). L√ºtfen AT Type Certificate y√ºkleyiniz.',
                        'details': {
                            'filename': filename,
                            'document_type': 'OTHER_REPORT_TYPE',
                            'required_type': 'AT_TYPE_CERTIFICATE'
                        }
                    }), 400
                else:
                    # A≈ûAMA 3: PyPDF2 ile tam dok√ºman kontrol√º
                    logger.info("A≈üama 3: Tam dok√ºman critical terms kontrol√º...")
                    try:
                        with open(filepath, 'rb') as pdf_file:
                            pdf_reader = PyPDF2.PdfReader(pdf_file)
                            text = ""
                            for page in pdf_reader.pages:
                                text += page.extract_text() + "\n"
                        
                        if not text or len(text.strip()) < 50:
                            try:
                                os.remove(filepath)
                            except:
                                pass
                            return jsonify({
                                'error': 'Text extraction failed',
                                'message': 'Dosyadan yeterli metin √ßƒ±karƒ±lamadƒ±'
                            }), 400
                        
                        if not validate_document_server(text, analyzer.validation_keywords):
                            try:
                                os.remove(filepath)
                            except:
                                pass
                            return jsonify({
                                'error': 'Invalid document type',
                                'message': 'Y√ºklediƒüiniz dosya AT Type Certificate deƒüil! L√ºtfen ge√ßerli bir AT Type Certificate y√ºkleyiniz.',
                                'details': {
                                    'filename': filename,
                                    'document_type': 'NOT_AT_TYPE_CERT',
                                    'required_type': 'AT_TYPE_CERTIFICATE'
                                }
                            }), 400
                            
                    except Exception as e:
                        logger.error(f"A≈üama 3 hatasƒ±: {e}")
                        try:
                            os.remove(filepath)
                        except:
                            pass
                        return jsonify({
                            'error': 'Analysis failed',
                            'message': 'Dosya analizi sƒ±rasƒ±nda hata olu≈ütu'
                        }), 500
        
        elif file_ext in ['.docx', '.doc', '.txt']:
            logger.info(f"DOCX/TXT dosyasƒ± i√ßin tam dok√ºman kontrol√º: {file_ext}")
            text = ""
            if file_ext in ['.docx', '.doc']:
                text = analyzer.extract_text_from_docx(filepath)
            elif file_ext == '.txt':
                text = analyzer.extract_text_from_txt(filepath)
            
            if not text or len(text.strip()) == 0:
                try:
                    os.remove(filepath)
                except:
                    pass
                return jsonify({
                    'error': 'Text extraction failed',
                    'message': 'Dosyadan yeterli metin √ßƒ±karƒ±lamadƒ±'
                }), 400
            
            if not validate_document_server(text, analyzer.validation_keywords):
                try:
                    os.remove(filepath)
                except:
                    pass
                return jsonify({
                    'error': 'Invalid document type',
                    'message': 'Y√ºklediƒüiniz dosya AT Type Certificate deƒüil! L√ºtfen ge√ßerli bir AT Type Certificate y√ºkleyiniz.',
                    'details': {
                        'filename': filename,
                        'document_type': 'NOT_AT_TYPE_CERT',
                        'required_type': 'AT_TYPE_CERTIFICATE'
                    }
                }), 400
        
        logger.info(f"AT Type Certificate doƒürulandƒ±, analiz ba≈ülatƒ±lƒ±yor: {filename}")
        report = analyzer.analyze_type_examination_certificate(filepath)
        
        try:
            os.remove(filepath)
        except:
            pass
        
        if 'error' in report:
            return jsonify({'error': 'Analysis failed', 'message': report['error']}), 400
        
        overall_percentage = report['summary']['percentage']
        status = "PASS" if overall_percentage >= 70 else "FAIL"
        
        # Extracted values'ƒ± T√ºrk√ße key'lerle d√∂n√º≈üt√ºr
        extracted_values_tr = {}
        display_names = {
            "notified_body_name": "Onaylanmƒ±≈ü Kurulu≈ü Adƒ±",
            "notified_body_address": "Onaylanmƒ±≈ü Kurulu≈ü Adresi",
            "notified_body_id": "Kurulu≈ü Kimlik No",
            "manufacturer_name": "ƒ∞malat√ßƒ± Adƒ±",
            "manufacturer_address": "ƒ∞malat√ßƒ± Adresi",
            "machine_trade_name": "Makinenin Ticari Adƒ±",
            "machine_type": "Makine Tipi",
            "machine_model": "Model",
            "serial_number": "Seri No",
            "certificate_number": "Belge Numarasƒ±",
            "issue_date": "D√ºzenlenme Tarihi",
            "validity_date": "Ge√ßerlilik S√ºresi",
            "directive_reference": "Direktif Atfƒ±",
            "applied_standards": "Uygulanan Standartlar",
            "authorized_person": "Yetkili Ki≈üi"
        }
        
        for eng_key, tr_name in display_names.items():
            if eng_key in report['extracted_values']:
                value = report['extracted_values'][eng_key]
                if eng_key == "applied_standards":
                    extracted_values_tr[tr_name] = ", ".join(value) if value else "Bulunamadƒ±"
                else:
                    extracted_values_tr[tr_name] = value
        
        response_data = {
            'analysis_date': report.get('analysis_date'),
            'analysis_id': f"at_type_cert_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'filename': filename,
            'file_type': 'AT_TIP_INCELEME_SERTIFIKASI',
            'overall_score': {
                'percentage': round(overall_percentage, 2),
                'total_points': report['summary']['total_score'],
                'max_points': 100,
                'status': status,
                'status_tr': report['summary']['status_tr']
            },
            'category_scores': {},
            'extracted_values': extracted_values_tr,
            'recommendations': report.get('recommendations', []),
            'summary': {
                'is_valid': status == "PASS",
                'conclusion': get_conclusion_message(status, overall_percentage),
                'main_issues': [] if status == "PASS" else get_main_issues(report)
            }
        }
        
        for category, score_data in report['scoring']['category_scores'].items():
            response_data['category_scores'][category] = {
                'score': score_data['normalized'],
                'max_score': score_data['max_weight'],
                'percentage': score_data['percentage'],
                'status': 'PASS' if score_data['percentage'] >= 70 else 'FAIL'
            }
        
        return jsonify({
            'success': True,
            'message': 'AT Type Certificate ba≈üarƒ±yla analiz edildi',
            'analysis_service': 'at_type_cert',
            'data': response_data
        })
    
    except Exception as e:
        logger.error(f"API endpoint hatasƒ±: {str(e)}")
        return jsonify({'error': 'Server error', 'message': str(e)}), 500


@app.route('/api/health', methods=['GET'])
def health_check():
    return jsonify({
        'status': 'healthy',
        'service': 'AT Type Certificate Analyzer API',
        'version': '1.0.0'
    })


@app.route('/', methods=['GET'])
def index():
    return jsonify({
        'service': 'AT Type Certificate Analyzer API',
        'version': '1.0.0'
    })


# ============================================
# DATABASE INITIALIZATION
# ============================================
with app.app_context():
    db.init_app(app)


# ============================================
# APPLICATION ENTRY POINT (Azure-Friendly)
# ============================================
if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8015))
    logger.info(f"üöÄ AT Type Certificate Analyzer API - Port: {port}")
    app.run(host='0.0.0.0', port=port, debug=False)